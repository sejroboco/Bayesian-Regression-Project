---
title: "Travaux Pratiques de Régression Bayesienne"
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: true
    toc_depth: 2
    number_sections: true
fontsize: 11pt
geometry: margin=2.5cm
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE)
```


\newpage

# Travaux Pratique 1

L’objectif de ce TP est de comparer le modèle de régression aléatoire, à la régression bayésienne, la régression bayésienne LASSO et la méthode de sélection bayésienne $SSVS$. Le modèle linéaire étudié est le suivant :

$$
Y = \mu \mathbb{I} + X \beta + \varepsilon
$$

## Simulation des données

Afin de comparer ces différentes méthodes, des données simulées seront utilisées. Pour cela, on génère un jeu de données de $200$ observation qui sont réparties en données d'apprentissage pour la construction des modèles et des données de test qui serviront à comparer les modèles en terme de performance. L'échantillon sera coupé en deux : $100$ observations pour l'apprentissage et $100$ observations pour le test. On générera $300$ variables indépendantes et identiquement distribuées ($i.i.d.$), chacune suivant une loi uniforme entre $-5$ et $5$.

```{r simulation-de-données, echo=TRUE, results='hide'}
simuVarExpl <- matrix(rep(0, 300*200), ncol=300, nrow=200)
for (i in 1 :300){
  simuVarExpl[,i] <- runif(200, -5, 5)
  simuVarExpl <- as.data.frame(simuVarExpl)
}
```

Dans le lot des variables indépendantes générées, nous n'utiliserons que cinq ($05$) pour la simulation de la variable d'intérêt. Il s'agira de celles d'indices $10$, $20$, $30$, $40$ et $50$. Nous utiliserons le vecteur de paramètres $\beta = \left(+1, -1, +2, -2, +3\right)$ et nous supposons que $\mu = 0$.

```{r simulation-de-y, echo=TRUE, results='hide'}
trueInd <- c(10, 20, 30, 40, 50)
beta <- c(1, -1, 2, -2, 3)
ySimu <- as.matrix(simuVarExpl)[,trueInd]%*% beta + rnorm(200,0,2)
```


## Mise en place des fonctions utiles

Deux principales fonctions seront utiles pour la réalisation de ce TP. Il s'agit des fonctions de prédiction de la variable d'intérêt et de sélection de variables explicatives.

### Prédictions

En se basant sur des variables explicatives et des estimateurs de $\mu$ et $\beta$, la fonction suivante permet de faire des prédictions de la variable d'intérêt.

```{r fonction-de-prediction, echo=TRUE, results='hide'}
predictions <- function(maTableTest, muChap, betaChap){
  yChap <- muChap * rep(1, dim(maTableTest)[1]) + as.matrix(maTableTest[,]) %*% betaChap
  return(yChap)
}
```

### Sélections

La fonction suivante sera utile pour la sélection de variables ; elle donne un sous-ensemble de variables pertinentes à sélectionner.

```{r fonction-de-selection, echo=TRUE, results='hide'}
subsetSelection <- function(resAlgo, varExpl, mini, maxi){
  numselected <- c(which(resAlgo < mini), which(resAlgo > maxi))
  selected <- character()
  valeurs <- numeric()
  for (i in 1 :length(numselected)) {
    selected[i] <- names(varExpl)[numselected[i]]
    valeurs[i] <- resAlgo[numselected[i]] }
    subset <- cbind(selected, valeurs)
    subset <- as.data.frame(subset)
    subset$valeurs <- as.numeric(as.vector(subset$valeurs))
  return(subset)
}
```


## Utilisation du package `rrBLUP`

### Estimation

La fonction `mixed.solve` du package `rrBLUP` nous permet d'estimer les paramètres $\beta$ et $\mu$ du modèle aléatoire

```{r estimation-modèle-aléatoire}
library(rrBLUP)
resBLUP <- mixed.solve(ySimu[1 :100], Z = as.matrix(simuVarExpl[1 :100, ]),
                       X = as.matrix(rep(1, 100)), method = "REML")
muChap <- resBLUP$beta
betaChap <- resBLUP$u
```

```{r estimation-variances}
resBLUP$Ve
resBLUP$Vu
```

L'estimation de la variance résiduelle vaut $16.43825$ ; ce qui révèle une forte variabilité au sein de celle-ci.

### Prédiction

En se servant des estimations obtenues à l'aide du modèle aléatoire, nous pouvons faire des prédictions de la variable d'intérêt pour les observations du jeu de test et comparer ces prédictions aux valeurs observées.

```{r}
predBLUP <- predictions(simuVarExpl[101 :200, ], muChap, betaChap)
cor(ySimu[101 :200], predBLUP)
plot(predBLUP ~ ySimu[101 :200])
```


### Sélection

## La régression bayésienne A

### Fonction

### Estimation

### Prédiction

### Sélection

## La régression bayésienne LASSO

### Estimation

### Prédiction

## La méthode de sélection bayésienne SSVS

