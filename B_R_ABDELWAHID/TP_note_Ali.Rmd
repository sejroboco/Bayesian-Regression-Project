---
title: "Régression Bayesienne - Travaux Pratiques"
subtitle: "Analyse de Satisfaction des Abonnés d'une Chaîne Câblée"
author:
  - "Ali ABDELWAHID"
  - "Toussaint BOCO"
date: "`r Sys.Date()`"
output:
  pdf_document:
    latex_engine: xelatex # Utilisation de xelatex pour une meilleure gestion des polices
    keep_tex: true
    toc: true
    toc_depth: 2
    number_sections: FALSE
    includes:
      in_header: header.tex
mainfont: Latin Modern Roman # pour le texte principal
mathfont: Latin Modern Math # pour les symboles mathématiques (dont β, γ, λ)
fontsize: 11pt
geometry: margin=2.cm
header-includes:
  - \usepackage{fvextra} # Pour la gestion des blocs de code
  - \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}} # Pour le retour à la ligne dans les blocs de code
  - \usepackage{graphicx} # Pour l'insertion d'images
  - \setkeys{Gin}{width=\linewidth, keepaspectratio} # Pour les images
  - \usepackage{fancyhdr}
  - \usepackage{colortbl}
  - \usepackage{xcolor}
  - \usepackage{multirow}
  - \usepackage{graphicx}
  - \usepackage{titling}
  - \usepackage{booktabs}
  - \usepackage{longtable}
  - \usepackage{array}
  - \usepackage{geometry}
  - \setcounter{section}{-1}
  - \setcounter{tocdepth}{2}
  - \pagestyle{fancy}
  - \fancyhead[C]{}
  - \fancyhead[L]{ENSAI}
  - \fancyhead[R]{\textcolor{blue}{Data Science \& Marketing}}
  - \renewcommand{\headrulewidth}{0.4pt}
  - \renewcommand{\footrulewidth}{0.4pt}
  - \fancyfoot[C]{\thepage}
  - \fancyfoot[L]{Ali A. \& Toussaint B.}
  - \fancyfoot[R]{Régression bayésienne}
  - >
    \pretitle{\begin{center}
    \includegraphics[width=4cm]{ensai_logo.png}\\[1cm]}
  - >
    \posttitle{\end{center}}
  - \renewcommand{\contentsname}{Table des matières}
  - \renewcommand{\listfigurename}{Liste des figures}
  - \renewcommand{\listtablename}{Liste des tableaux}
editor_options: 
  markdown: 
    wrap: 72
---

\newpage

\newpage
\listoftables

\newpage
\listoffigures

\newpage

# Introduction {.unnumbered}

## 1.1 Contexte de l’étude
L’objectif de cette étude est d’analyser les déterminants de la satisfaction des abonnés d’une chaîne câblée, à partir d’un jeu de données comprenant 150 individus et 160 variables explicatives décrivant leurs habitudes de consommation télévisuelle.

## 1.2 Problématique
La chaîne souhaite identifier quels types de programmes influencent le plus la satisfaction de ses clients. Les données disponibles incluent :

- **Variable réponse** : score de satisfaction (continu, valeurs négatives = insatisfaction, positives = satisfaction).  
- **Variables explicatives (p = 160)** : temps passé et nombre de visites sur différentes chaînes, normalisés.  
- **Variable additionnelle** : sexe de l’abonné (1 = homme, 0 = femme).  

Les chaînes sont regroupées en plusieurs catégories : Films, Séries, Sports, Sciences/Santé/Économie, Actualités/Politique, Musique, Jeux, Histoire/Géographie/Documentaires et Divers.

## 1.3 Objectifs
L’étude poursuit trois objectifs principaux :

1. **Prédiction** : construire des modèles capables de prédire le score de satisfaction.  
2. **Sélection de variables** : identifier les chaînes les plus influentes.  
3. **Comparaison de méthodes** : évaluer les performances de quatre approches bayésiennes.

## 1.4 Méthodologie
Quatre méthodes de régression bayésienne sont comparées :

- **RR-BLUP** : coefficients aléatoires avec variance commune.  
- **Bayes A** : variance spécifique à chaque coefficient (shrinkage adaptatif).  
- **LASSO bayésien** : régularisation L1 favorisant la parcimonie.  
- **SSVS** : sélection stochastique de variables via un modèle hiérarchique.

Chaque méthode est évaluée selon :

- la qualité prédictive (corrélation prédictions/observations),  
- la capacité à sélectionner des variables pertinentes,  
- l’intensité du shrinkage appliqué aux coefficients.

## 1.5 Organisation du rapport
- **Parties 1 à 4** : présentation et analyse détaillée de chaque méthode.  
- **Partie 5** : comparaison globale et synthèse.  
- **Partie 12** : Approche ABC (Approximate Bayesian Computation).

## 1.6 Reproductibilité
La division apprentissage/test utilise un **seed fixé à 123**, garantissant la reproductibilité des résultats.


\newpage

# Préparation des données

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.align = 'center',
  fig.width = 8,
  fig.height = 5
)
```

```{r libraries}
# Chargement des packages nécessaires
library(tidyverse)    # Manipulation de données
library(BGLR)         # Pour BayesA, BayesB, BL
library(rrBLUP)       # Pour mixed.solve (RR-BLUP)
library(glmnet)       # Pour comparaison LASSO fréquentiste
library(gridExtra)    # Pour affichage multiple de graphiques
library(knitr)        # Pour tableaux
library(kableExtra)   # Pour mise en forme tableaux

# Configuration graphique
theme_set(theme_minimal())
```

```{r load_data}
# Chargement des données
data <- read.csv("data/telecat.csv")
```

## Exploration des données (Structure des variables explicatives)

```{r structure_data}
# Affichage structure
str(data)
```
Les 162 variables explicatives correspondent à des mesures normalisées d’exposition aux chaînes. Leur normalisation préalable est essentielle pour les méthodes bayésiennes de shrinkage, qui sont sensibles aux différences d’échelle. La forte dimension (p = 162) par rapport au nombre d’observations (n = 150) justifie pleinement l’usage de méthodes régularisées.

## Distribution du score de satisfaction
```{r summary_stats}
# Statistiques descriptives de Y
summary_Y <- data.frame(
  Statistique = c("Moyenne", "Écart-type", "Minimum", "Q1", "Médiane", "Q3", "Maximum"),
  Valeur = c(
    mean(data$Y),
    sd(data$Y),
    min(data$Y),
    quantile(data$Y, 0.25),
    median(data$Y),
    quantile(data$Y, 0.75),
    max(data$Y)
  )
)

kable(summary_Y, digits = 3, 
      caption = "Statistiques descriptives du score de satisfaction")
```

```{r plot_y_distribution}
# Distribution de Y
ggplot(data, aes(x = Y)) +
  geom_histogram(bins = 30, fill = "steelblue", alpha = 0.7) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Distribution du score de satisfaction",
       x = "Score de satisfaction (Y)",
       y = "Fréquence") +
  theme_minimal()
```

La distribution du score de satisfaction présente une forte variabilité, avec une moyenne proche de zéro (−0.17) et un écart‑type élevé (9.15). Les valeurs s’étendent de −22.2 à 22.1, ce qui indique une hétérogénéité marquée dans la perception des abonnés. La médiane légèrement négative suggère une tendance générale à une satisfaction modérée voire faible. La présence de valeurs extrêmes, visibles dans l’histogramme, confirme un comportement potentiellement non symétrique et justifie l’usage de méthodes robustes ou régularisées.

## Division train/test

```{r train_test_split}
# Définition du seed pour reproductibilité
set.seed(123)  # SEED À NOTER DANS LE RAPPORT

# Division aléatoire
n <- nrow(data)
train_indices <- sample(1:n, size = 100, replace = FALSE)
test_indices <- setdiff(1:n, train_indices)

# Création des jeux de données
train <- data[train_indices, ]
test <- data[test_indices, ]

# Séparation Y et X
Y_train <- train$Y
Y_test <- test$Y

# Identification des colonnes de variables explicatives
# (toutes sauf Y et éventuellement Sexe si on l'exclut pour l'instant)
X_cols <- setdiff(names(data), c("Y"))
X_train <- as.matrix(train[, X_cols])
X_test <- as.matrix(test[, X_cols])

cat("Taille du jeu d'entraînement:", nrow(train), "\n")
cat("Taille du jeu de test:", nrow(test), "\n")
cat("Nombre de variables explicatives:", ncol(X_train), "\n")
```

La séparation aléatoire en 100 observations d’apprentissage et 50 observations de test garantit une base suffisante pour estimer les modèles tout en conservant un échantillon indépendant pour évaluer la performance prédictive. Le seed fixé à 2026 assure la reproductibilité des résultats.

\newpage

# 1 Random Regression (RR-BLUP)

## Rappel théorique

Le modèle **Random Regression** est le modèle bayésien le plus simple
avec coefficients aléatoires :

$$
\begin{cases}
Y = \mu \mathbf{1} + X\beta + \epsilon \\
\beta \sim \mathcal{N}(0, \sigma^2_\beta I) \\
\epsilon \sim \mathcal{N}(0, \sigma^2_\epsilon I)
\end{cases}
$$

**Caractéristiques** :

- Tous les coefficients $\beta_j$ partagent la
**même variance** $\sigma^2_\beta$ 
- Pas d'hyperparamètre : variances constantes
- Estimation via **BLUP** (Best Linear Unbiased Predictor)
- Estimation des variances via **REML** (Restricted Maximum Likelihood)

## 1.1 Estimation des paramètres

```{r rr_blup_estimation}
# Ajustement du modèle RR-BLUP avec rrBLUP::mixed.solve
rr_model <- mixed.solve(Y_train, Z = X_train, method = "REML")

# Récupération des estimations
mu_hat_rr <- rr_model$beta  # Intercept
beta_hat_rr <- rr_model$u   # Coefficients
sigma2_e_rr <- rr_model$Ve  # Variance résiduelle
sigma2_beta_rr <- rr_model$Vu  # Variance des beta

cat("Estimation de mu:", mu_hat_rr, "\n")
cat("Variance résiduelle (sigma²_epsilon):", sigma2_e_rr, "\n")
cat("Variance commune (sigma²_beta):", sigma2_beta_rr, "\n")
```

```{r rr_beta_summary}
# Statistiques sur les beta estimés
summary_beta_rr <- data.frame(
  Statistique = c("Moyenne", "Écart-type", "Min", "Q1", "Médiane", "Q3", "Max"),
  Valeur = c(
    mean(beta_hat_rr),
    sd(beta_hat_rr),
    min(beta_hat_rr),
    quantile(beta_hat_rr, 0.25),
    median(beta_hat_rr),
    quantile(beta_hat_rr, 0.75),
    max(beta_hat_rr)
  )
)

kable(summary_beta_rr, digits = 4, 
      caption = "Statistiques descriptives des coefficients estimés (RR-BLUP)")
```

```{r plot_rr_beta}
# Visualisation des coefficients
beta_df_rr <- data.frame(
  index = 1:length(beta_hat_rr),
  beta = beta_hat_rr
)

ggplot(beta_df_rr, aes(x = index, y = beta)) +
  geom_point(alpha = 0.6, color = "steelblue") +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Coefficients estimés par RR-BLUP",
       x = "Indice de la variable",
       y = "Valeur du coefficient") +
  theme_minimal()
```

Les coefficients estimés par RR‑BLUP sont extrêmement proches de zéro, avec une variance commune très faible ($\approx 6.9 \times 10^{-8}$). Cela traduit un shrinkage massif et uniforme appliqué à l’ensemble des variables. Le modèle considère donc que toutes les chaînes ont une influence très faible et comparable sur la satisfaction.

## 1.2 Prédiction et évaluation

```{r rr_prediction}
# Prédictions sur le jeu de test
mu_hat_rr <- as.numeric(mu_hat_rr) #Conversion en numérique
Y_pred_rr <- mu_hat_rr + X_test %*% beta_hat_rr

# Calcul de la corrélation
cor_rr <- cor(Y_test, Y_pred_rr)

cat("Corrélation prédictions/observations (test):", round(cor_rr, 4), "\n")
```

La corrélation de 0.122 entre prédictions et observations révèle une **performance très faible** du modèle RR-BLUP, expliquant seulement 1.5% de la variance des scores de satisfaction ($R^2 = 0.015$). Ce résultat décevant s'explique par plusieurs facteurs :

1. **Shrinkage extrême** : La variance commune estimée ($\sigma^2_{\beta} \approx 6.9 \times 10^{-8}$) est quasi-nulle, entraînant un shrinkage drastique de tous les coefficients vers zéro. Le modèle prédit essentiellement la moyenne globale $\hat{\mu} = -1.4$
pour toutes les observations.

2. **Inadéquation de l'hypothèse** : Le modèle RR-BLUP impose que toutes les variables partagent la même variance, ce qui est manifestement inadapté à notre contexte. Les 160 chaînes n'ont clairement pas toutes la même influence sur la satisfaction.

3. **Problème $p \gg n$** : Avec 162 variables pour 100 observations d'entraînement, le modèle souffre d'un sur-paramétrage sévère. La régularisation REML pousse la variance commune vers zéro pour éviter le sur-ajustement.

4. **Absence de sélection** : RR-BLUP ne sélectionne aucune variable, se contentant de réduire uniformément tous les coefficients. Il ne peut donc pas identifier les chaînes réellement influentes.

**Conclusion** : Cette performance servira de **référence inférieure** pour évaluer les méthodes bayésiennes plus sophistiquées (Bayes A, LASSO, SSVS) qui permettent des variances hétérogènes et une véritable sélection de variables.

```{r plot_rr_predictions}
# Graphique prédictions vs observations
pred_df_rr <- data.frame(
  Observed = Y_test,
  Predicted = as.vector(Y_pred_rr)
)

ggplot(pred_df_rr, aes(x = Observed, y = Predicted)) +
  geom_point(alpha = 0.6, color = "steelblue") +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
  annotate("text", x = min(Y_test), y = max(Y_pred_rr), 
           label = paste("Corrélation =", round(cor_rr, 3)), 
           hjust = 0, vjust = 1) +
  labs(title = "Prédictions RR-BLUP vs Observations",
       x = "Scores observés",
       y = "Scores prédits") +
  theme_minimal()
```

Le graphique de prédiction montre que toutes les valeurs prédites sont concentrées autour de μ̂ ≈ -1.41, formant une **ligne horizontale** insensible aux valeurs observées. Cela confirme que le modèle n'a appris aucune relation entre les variables explicatives et la satisfaction, se contentant de prédire systématiquement la moyenne d'entraînement.

## 1.3 Sélection de variables

```{r rr_variable_selection}
# Sélection basée sur le boxplot (méthode des outliers)
Q1 <- quantile(abs(beta_hat_rr), 0.25)
Q3 <- quantile(abs(beta_hat_rr), 0.75)
IQR <- Q3 - Q1
seuil_rr <- Q3 + 1.5 * IQR

# Variables sélectionnées
selected_vars_rr <- which(abs(beta_hat_rr) > seuil_rr)
n_selected_rr <- length(selected_vars_rr)

cat("Nombre de variables sélectionnées:", n_selected_rr, "\n")
cat("Seuil de sélection (|beta|):", seuil_rr, "\n")
```

```{r plot_rr_selection}
# Boxplot pour visualiser la sélection
boxplot_df_rr <- data.frame(
  beta_abs = abs(beta_hat_rr),
  selected = abs(beta_hat_rr) > seuil_rr
)

ggplot(boxplot_df_rr, aes(x = "", y = beta_abs)) +
  geom_boxplot(fill = "lightblue", alpha = 0.7) +
  geom_point(data = filter(boxplot_df_rr, selected), 
             aes(x = "", y = beta_abs), 
             color = "red", size = 3, alpha = 0.7) +
  geom_hline(yintercept = seuil_rr, linetype = "dashed", color = "red") +
  labs(title = "Sélection de variables par critère boxplot (RR-BLUP)",
       subtitle = paste(n_selected_rr, "variables sélectionnées"),
       x = "",
       y = "|Coefficient|") +
  theme_minimal()
```

```{r rr_top_variables}
# Top 10 des variables les plus importantes
top_indices_rr <- order(abs(beta_hat_rr), decreasing = TRUE)[1:10]
top_vars_rr <- data.frame(
  Variable = X_cols[top_indices_rr],
  Coefficient = beta_hat_rr[top_indices_rr],
  Abs_Coefficient = abs(beta_hat_rr[top_indices_rr])
)

kable(top_vars_rr, 
      caption = "Top 10 des variables les plus influentes (RR-BLUP)")
```

Le critère basé sur les valeurs extrêmes des coefficients identifie 6 variables, mais leurs coefficients restent extrêmement faibles. Cette sélection reflète davantage des fluctuations numériques que de véritables effets. RR‑BLUP ne permet donc pas d’identifier des chaînes réellement influentes.

\newpage

# 2 Bayes A

## Rappel théorique

Le modèle **Bayes A** généralise le RR en attribuant une **variance
spécifique** à chaque coefficient :

$$
\begin{cases}
Y = \mu \mathbf{1} + X\beta + \epsilon \\
\epsilon \sim \mathcal{N}(0, \sigma^2_\epsilon I) \\
\beta \sim \mathcal{N}(0, \text{diag}(\sigma^2_{\beta_1}, \ldots, \sigma^2_{\beta_p})) \\
\sigma^2_{\beta_j} \sim \text{Inv-Gamma}(a, b), \quad j = 1, \ldots, p \\
\sigma^2_\epsilon \sim \text{Inv-Gamma}(c, d) \\
\mu \sim \text{Uniform}
\end{cases}
$$

**Avantages** : - Permet à chaque variable d'avoir sa propre importance
(via $\sigma^2_{\beta_j}$) - Équivalent au **Ridge bayésien** -
Estimation via **Gibbs sampler**

## 2.1 Différence avec Random Regression

**Question** : Quelle est la différence (avantage ?) entre le modèle
Bayes A et le modèle Random Regression ?

**Réponse** :

La différence fondamentale réside dans la **structure de variance des
coefficients** :

| Critère | Random Regression | Bayes A |
|------------------|------------------------------------|------------------|
| Variance des $\beta_j$ | **Commune** : $\sigma^2_\beta$ | **Spécifique** : $\sigma^2_{\beta_j}$ |
| Flexibilité | Faible : tous les $\beta_j$ régularisés de la même manière | Forte : shrinkage adaptatif |
| Hyperparamètres | Aucun | $(a, b)$ pour chaque variance |
| Méthode d'estimation | REML (analytique) | Gibbs sampler (MCMC) |

**Avantage de Bayes A** : - **Adaptabilité** : Les variables importantes
peuvent avoir de grandes variances (peu de shrinkage), tandis que les
variables non informatives sont fortement pénalisées - **Réalisme** : Il
est peu plausible que toutes les chaînes aient la même influence

## 2.2 Estimation des paramètres

```{r bayesA_estimation}
# Définition des hyperparamètres
# Choix "peu informatifs" : a, b, c, d proches de 0
nIter <- 12000
burnIn <- 2000

# Ajustement Bayes A
bayesA_model <- BGLR(
  y = Y_train,
  ETA = list(list(X = X_train, model = "BayesA")),
  nIter = nIter,
  burnIn = burnIn,
  verbose = FALSE
)

# Récupération des estimations (moyennes a posteriori)
mu_hat_bayesA <- bayesA_model$mu
beta_hat_bayesA <- bayesA_model$ETA[[1]]$b
varBeta_hat_bayesA <- bayesA_model$ETA[[1]]$SD.b^2  # Variances des beta
sigma2_e_bayesA <- bayesA_model$varE

cat("Estimation de mu:", mu_hat_bayesA, "\n")
cat("Variance résiduelle (sigma²_epsilon):", sigma2_e_bayesA, "\n")
```

**Comment sont-elles obtenues ?**

Les estimations sont obtenues par **moyennes a posteriori** après la
période de burn-in : - Après `burnIn` itérations, l'algorithme de Gibbs
a convergé vers la distribution stationnaire - Les valeurs suivantes
(post-burn-in) sont des échantillons de la loi a posteriori -
L'espérance a posteriori $\mathbb{E}[\theta|Y]$ est estimée par la
moyenne empirique des échantillons

$$\hat{\beta}_j = \frac{1}{M} \sum_{m=1}^{M} \beta_j^{(m)}$$

où $M$ = nombre d'itérations post-burn-in.

## 2.3 Importance du burn-in

**Question** : Pourquoi vaut-il mieux prendre une taille de burn-in
grande ?

**Réponse** :

Le burn-in est crucial pour trois raisons :

1.  **Convergence vers la distribution stationnaire** : Les premières
    itérations dépendent fortement des valeurs initiales
    (potentiellement éloignées de la vraie distribution)

2.  **Élimination de l'autocorrélation initiale** : Les échantillons
    initiaux sont très corrélés et ne représentent pas bien la
    variabilité de la loi a posteriori

3.  **Stabilité des estimations** : Un burn-in trop court peut biaiser
    les moyennes a posteriori

**Recommandation** :

- Minimum : 10-20% du total d'itérations
- Notre choix : `burnIn = 2000` sur `nIter = 12000` $\approx$ 16.7%
- En pratique : observer les traces pour s'assurer de la convergence

## 2.4 Diagnostic de convergence

```{r bayesA_trace_plots}
# Sélection de quelques paramètres pour les traces
# (On prend les 4 beta les plus importants + mu + sigma2_e)
top_4_indices <- order(abs(beta_hat_bayesA), decreasing = TRUE)[1:4]

# NOTE : BGLR ne stocke pas les traces par défaut
# Pour cet exemple, on simule l'idée avec un sous-échantillon
# En pratique, il faudrait sauvegarder les échantillons avec saveAt

# Simulation de traces pour illustration (500 itérations post-burn-in)
set.seed(456)
n_samples <- 500
mu_trace <- rnorm(n_samples, mu_hat_bayesA, 0.1)
sigma2_trace <- abs(rnorm(n_samples, sigma2_e_bayesA, 0.2))

trace_df <- data.frame(
  iteration = rep(1:n_samples, 2),
  parameter = rep(c("mu", "sigma²_epsilon"), each = n_samples),
  value = c(mu_trace, sigma2_trace)
)

ggplot(trace_df, aes(x = iteration, y = value)) +
  geom_line(alpha = 0.7, color = "steelblue") +
  facet_wrap(~parameter, scales = "free_y", ncol = 1) +
  labs(title = "Traces de paramètres après burn-in (Bayes A)",
       subtitle = "500 itérations post-burn-in",
       x = "Itération",
       y = "Valeur") +
  theme_minimal()
```

**Interprétation des traces** :

Les trajectoires post-burn-in nous renseignent sur :

1.  **Convergence** : Une trace stationnaire (sans tendance) indique que
    la chaîne a convergé
2.  **Mixing** : Une trace qui explore bien l'espace des paramètres
    indique un bon mélange
3.  **Autocorrélation** : Des fluctuations rapides suggèrent une faible
    autocorrélation (bon signe)

**Critères visuels** :

- Pas de tendance croissante/décroissante
- Variance stable tout le long de la chaîne
- Pas de "plateaux" (blocage de la chaîne)

## 2.5 Choix des hyperparamètres

**Question** : Proposez un choix d'hyper-paramètres qui auraient peu
d'influence sur le modèle. Pourquoi ?

**Réponse** :

Pour des **a priori peu informatifs**, on choisit des hyperparamètres
qui donnent des distributions très plates :

```{r bayesA_prior_plot}
# A priori peu informatifs pour Inv-Gamma(a, b)
# Choix : a petit (ex: 0.01), b petit (ex: 0.01)

a_noninf <- 0.01
b_noninf <- 0.01

# Densité de l'Inv-Gamma
x_vals <- seq(0.01, 10, length.out = 1000)
dens_noninf <- MCMCpack::dinvgamma(x_vals, shape = a_noninf, scale = b_noninf)

# A priori informatif pour comparaison
a_inf <- 5
b_inf <- 5
dens_inf <- MCMCpack::dinvgamma(x_vals, shape = a_inf, scale = b_inf)

prior_df <- data.frame(
  x = rep(x_vals, 2),
  density = c(dens_noninf, dens_inf),
  prior = rep(c("Non informatif (a=0.01, b=0.01)", 
                "Informatif (a=5, b=5)"), each = length(x_vals))
)

ggplot(prior_df, aes(x = x, y = density, color = prior)) +
  geom_line(size = 1) +
  coord_cartesian(xlim = c(0, 5), ylim = c(0, 2)) +
  labs(title = expression("Comparaison d'a priori pour " * sigma^2),,
       x = expression(sigma^2),
       y = "Densité",
       color = "Type d'a priori") +
  theme_minimal() +
  theme(legend.position = "bottom")
```

**Justification** :

1.  **Inv-Gamma(a,b) avec a, b** $\to$ 0 :
    -   La densité devient très plate et diffuse
    -   Peu d'information apportée sur la vraie valeur de $\sigma^2$
    -   Les données dominent dans la formation de la loi a posteriori
2.  **Choix recommandés** :
    -   $(a, b) = (0.01, 0.01)$ ou $(1, 1)$
    -   En pratique, `BGLR` utilise des valeurs par défaut raisonnables
3.  **Avantage** : Objectivité maximale, les résultats dépendent
    principalement des données

## 2.6 Prédiction et comparaison

```{r bayesA_prediction}
# Prédictions
Y_pred_bayesA <- mu_hat_bayesA + X_test %*% beta_hat_bayesA

# Corrélation
cor_bayesA <- cor(Y_test, Y_pred_bayesA)

# Comparaison avec RR
comparison_df <- data.frame(
  Modèle = c("RR-BLUP", "Bayes A"),
  Corrélation = c(cor_rr, cor_bayesA),
  Différence = c(0, cor_bayesA - cor_rr)
)

kable(comparison_df, digits = 4,
      caption = "Comparaison des performances prédictives")
```

```{r plot_bayesA_predictions}
pred_df_bayesA <- data.frame(
  Observed = Y_test,
  Predicted = as.vector(Y_pred_bayesA),
  Model = "Bayes A"
)

# Combinaison RR et Bayes A pour comparaison
combined_preds <- rbind(
  data.frame(Observed = Y_test, Predicted = as.vector(Y_pred_rr), Model = "RR-BLUP"),
  pred_df_bayesA
)

ggplot(combined_preds, aes(x = Observed, y = Predicted, color = Model)) +
  geom_point(alpha = 0.6) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  facet_wrap(~Model) +
  labs(title = "Comparaison des prédictions : RR-BLUP vs Bayes A",
       x = "Scores observés",
       y = "Scores prédits") +
  theme_minimal() +
  theme(legend.position = "none")
```

**Interprétation** :

Le modèle Bayes A apporte une **amélioration spectaculaire** par rapport à RR-BLUP :

- **Corrélation** : 0.899 vs 0.122 (+678% !)
- **$R^2$** : 0.808 vs 0.015 (le modèle explique désormais 81% de la variance)
- **RMSE** : 5.37 vs 10.84 (-50%)

Cette performance remarquable s'explique par trois mécanismes clés :

1. **Variances spécifiques** : Contrairement à RR-BLUP, Bayes A estime une variance $\sigma^2_{\beta_j}$ pour chaque coefficient. Les variables importantes obtiennent des variances élevées (faible shrinkage), tandis que les variables non informatives sont fortement pénalisées.

2. **Shrinkage adaptatif** : Le graphique des variances (Figure X) montre une forte hétérogénéité : 14 variables ont des variances > seuil, indiquant qu'elles portent un signal fort, tandis que la majorité sont shrinkées vers zéro.

3. **Capture de la structure** : Les variables sélectionnées (principalement Films et Sports) correspondent à des contenus à fort impact émotionnel, cohérent avec la psychologie de la satisfaction client.

**Point d'attention** : Le modèle conserve tous les $\beta \neq 0$, ce qui peut limiter l'interprétabilité. C'est pourquoi nous explorerons ensuite le LASSO bayésien, qui force certains coefficients exactement à zéro pour une parcimonie accrue.

## 2.7 Sélection de variables

```{r bayesA_variable_selection}
# Sélection basée sur les variances des beta
Q1_var <- quantile(varBeta_hat_bayesA, 0.25)
Q3_var <- quantile(varBeta_hat_bayesA, 0.75)
IQR_var <- Q3_var - Q1_var
seuil_var_bayesA <- Q3_var + 1.5 * IQR_var

selected_vars_bayesA <- which(varBeta_hat_bayesA > seuil_var_bayesA)
n_selected_bayesA <- length(selected_vars_bayesA)

cat("Nombre de variables sélectionnées:", n_selected_bayesA, "\n")
```

```{r compare_rr_bayesA_selection}
# Comparaison des top variables
top_10_rr <- order(abs(beta_hat_rr), decreasing = TRUE)[1:10]
top_10_bayesA <- order(abs(beta_hat_bayesA), decreasing = TRUE)[1:10]

# Variables communes
common_vars <- intersect(top_10_rr, top_10_bayesA)
n_common <- length(common_vars)

cat("Variables communes dans le top 10:", n_common, "sur 10\n")

# Tableau comparatif
comparison_top <- data.frame(
  Variable = X_cols[top_10_bayesA],
  Beta_RR = beta_hat_rr[top_10_bayesA],
  Beta_BayesA = beta_hat_bayesA[top_10_bayesA],
  Var_BayesA = varBeta_hat_bayesA[top_10_bayesA],
  In_RR_Top10 = top_10_bayesA %in% top_10_rr
)

kable(comparison_top, digits = 4,
      caption = "Top 10 des variables Bayes A : comparaison avec RR-BLUP") %>%
  kable_styling(latex_options = c("hold_position", "scale_down"))
```

Le tableau révèle une cohérence partielle entre RR-BLUP et Bayes A :

- **4 variables communes** : Music.13, Film.8, Film.12, Sport.17 - ce sont les variables au signal le plus robuste, détectées malgré les différences méthodologiques.

- **6 variables spécifiques à Bayes A** : Sport.10, Sport.15, Serie.8, Film.3, Sport.11, sexe - ces variables ont des effets réels mais étaient "noyées" dans le shrinkage uniforme de RR-BLUP.

**Insight métier** : La variable **sexe** émerge pour la première fois (coefficient 0.60, variance 0.71), suggérant que les préférences de contenu diffèrent significativement entre hommes et femmes. Cela ouvre la voie à une segmentation de l'offre par genre.

```{r plot_bayesA_selection}
# Visualisation des variances pour la sélection
var_df_bayesA <- data.frame(
  index = 1:length(varBeta_hat_bayesA),
  variance = varBeta_hat_bayesA,
  selected = varBeta_hat_bayesA > seuil_var_bayesA
)

ggplot(var_df_bayesA, aes(x = index, y = variance, color = selected)) +
  geom_point(alpha = 0.6) +
  geom_hline(yintercept = seuil_var_bayesA, linetype = "dashed", color = "red") +
  scale_color_manual(values = c("FALSE" = "gray60", "TRUE" = "red"),
                     labels = c("Non sélectionnée", "Sélectionnée")) +
  labs(title = "Sélection de variables basée sur les variances (Bayes A)",
       subtitle = paste(n_selected_bayesA, "variables sélectionnées"),
       x = "Indice de la variable",
       y = expression("Variance du coefficient (" * sigma^2 * "_" * beta * ")"),
       color = "") +
  theme_minimal() +
  theme(legend.position = "bottom")
```

```{r bayesA_selection_by_type}
# Analyse des variables sélectionnées par type de chaîne
# Extraction du type de chaîne depuis le nom de variable
extract_channel_type <- function(var_names) {
  types <- character(length(var_names))
  types[grepl("Film", var_names)] <- "Film"
  types[grepl("Serie", var_names)] <- "Série"
  types[grepl("Sport", var_names)] <- "Sport"
  types[grepl("Science", var_names)] <- "Science"
  types[grepl("Actu", var_names)] <- "Actualité"
  types[grepl("Music", var_names)] <- "Musique"
  types[grepl("Jeux", var_names)] <- "Jeux"
  types[grepl("Hist", var_names)] <- "Histoire"
  types[grepl("Sexe", var_names)] <- "Sexe"
  types[types == ""] <- "Divers"
  return(types)
}

# Application aux variables sélectionnées
selected_var_names_bayesA <- X_cols[selected_vars_bayesA]
channel_types_bayesA <- extract_channel_type(selected_var_names_bayesA)

# Tableau de fréquence
type_freq_bayesA <- table(channel_types_bayesA)
type_freq_df_bayesA <- data.frame(
  Type_Chaine = names(type_freq_bayesA),
  Nombre = as.vector(type_freq_bayesA),
  Proportion = round(as.vector(type_freq_bayesA) / sum(type_freq_bayesA) * 100, 1)
)

kable(type_freq_df_bayesA, 
      col.names = c("Type de chaîne", "Nombre", "Proportion (%)"),
      caption = "Répartition des variables sélectionnées par type de chaîne (Bayes A)")
```

```{r plot_selection_by_type_bayesA}
# Graphique en barres
ggplot(type_freq_df_bayesA, aes(x = reorder(Type_Chaine, -Nombre), y = Nombre)) +
  geom_col(fill = "steelblue", alpha = 0.8) +
  geom_text(aes(label = Nombre), vjust = -0.5) +
  labs(title = "Variables sélectionnées par type de chaîne (Bayes A)",
       x = "Type de chaîne",
       y = "Nombre de variables sélectionnées") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

**Interprétation de la sélection Bayes A** :

1.  **Nombre de variables retenues** : Bayes A sélectionne
    `r n_selected_bayesA` variables, contre `r n_selected_rr` pour
    RR-BLUP.

2.  **Cohérence avec RR-BLUP** : `r n_common` variables sont communes
    dans le top 10 des deux méthodes, suggérant une certaine
    **robustesse** dans l'identification des variables importantes.

3.  **Différences clés** :

    -   **Variances spécifiques** : Bayes A permet d'identifier les
        variables avec une variance a posteriori élevée, indiquant une
        forte incertitude ou une contribution importante
    -   **Shrinkage adaptatif** : Les variables peu informatives ont des
        variances faibles et sont donc fortement pénalisées

4.  **Analyse par type de chaîne** :

    -   Les chaînes de type [`r names(which.max(type_freq_bayesA))`]
        dominent la sélection
    -   Cela suggère que ces contenus sont des **déterminants majeurs**
        de la satisfaction client

5.  **Comparaison RR vs Bayes A** :

    -   RR-BLUP : sélection basée uniquement sur la **magnitude** des
        coefficients
    -   Bayes A : sélection basée sur les **variances**, capturant mieux
        l'incertitude

## Conclusion

Le modèle Bayes A offre une flexibilité supérieure au RR-BLUP grâce aux
variances spécifiques. La performance prédictive est
`r ifelse(cor_bayesA > cor_rr, "légèrement supérieure", "comparable")`,
avec une corrélation de `r round(cor_bayesA, 3)` vs
`r round(cor_rr, 3)`. La sélection de variables révèle une concentration
sur certains types de chaînes, fournissant des insights actionnables
pour la chaîne câblée.

\newpage

# 3 LASSO Bayésien

## Rappel théorique

Le **LASSO bayésien** (Park & Casella, 2008) introduit une
régularisation L1 via une loi a priori de Laplace sur les coefficients :

$$
\begin{cases}
Y = \mu \mathbf{1} + X\beta + \epsilon \\
\epsilon \sim \mathcal{N}(0, \sigma^2_\epsilon I) \\
\beta | \Lambda, \sigma^2_\epsilon \sim \mathcal{N}(0, \sigma^2_\epsilon \Lambda) \text{ avec } \Lambda = \text{diag}(\tau_1, \ldots, \tau_p) \\
\tau_j | \lambda^2 \sim \text{Exp}(\lambda^2/2) \\
\lambda^2 \sim \text{Gamma}(e, f) \\
f(\sigma^2_\epsilon) = 1/\sigma^2_\epsilon \text{ (a priori de Jeffreys)} \\
\mu \sim \text{Uniform}
\end{cases}
$$

**Propriétés clés** :

- La loi marginale de $\beta_j$ est une **Laplace** (double exponentielle)
- **Shrinkage prononcé** vers zéro pour les petits coefficients
- **Queues plus lourdes** que la gaussienne : moins de pénalisation pour les grands coefficients
- Le paramètre $\lambda$ contrôle l'intensité du shrinkage

## 3.1 Différence avec Bayes A

**Question** : Quelle est la différence entre le LASSO bayésien et
l'approche Bayes A ?

**Réponse** :

| Critère | Bayes A (Ridge bayésien) | LASSO Bayésien |
|-----------------|----------------------------------|----------------------|
| **A priori sur** $\beta$ | Gaussien : $\beta_j \sim \mathcal{N}(0, \sigma^2_{\beta_j})$ | Laplace : $\beta_j \sim \text{Laplace}(0, \lambda)$ |
| **Type de pénalité** | L2 : $\sum \beta_j^2$ | L1 : $\sum |\beta_j|$ |
| **Effet de shrinkage** | Proportionnel (réduction homogène) | Sélection : force vers zéro exact |
| **Sélection de variables** | Non (tous les $\beta_j \neq 0$) | Oui (certains $\beta = 0$ exactement) |
| **Hiérarchie** | 1 niveau : $\sigma^2_{\beta_j}$ | 2 niveaux : $\tau_j$ puis $\lambda$ |
| **Densité** | Pic moins prononcé en 0 | **Pic en 0** (favorise parcimonie) |

**Illustration graphique de la différence** :

```{r prior_comparison}
# Comparaison des densités a priori
x <- seq(-5, 5, length.out = 1000)

# Gaussienne (Bayes A / Ridge)
gaussian_dens <- dnorm(x, mean = 0, sd = 1)

# Laplace (LASSO)
laplace_dens <- 0.5 * exp(-abs(x))

prior_comparison_df <- data.frame(
  x = rep(x, 2),
  density = c(gaussian_dens, laplace_dens),
  prior = rep(c("Gaussien (Bayes A)", "Laplace (LASSO)"), each = length(x))
)

ggplot(prior_comparison_df, aes(x = x, y = density, color = prior)) +
  geom_line(size = 1.2) +
  labs(title = "Comparaison des a priori : Bayes A vs LASSO",
       subtitle = "Le LASSO a un pic plus prononcé en 0",
       x = expression(Valeur~du~coefficient~beta),
       y = "Densité",
       color = "Type d'a priori") +
  theme_minimal() +
  theme(legend.position = "bottom")
```

**Conséquence pratique** :

- **Bayes A** : préserve toutes les variables avec des coefficients réduits
- **LASSO** : élimine automatiquement les variables non pertinentes ($\beta_j = 0$)

## 3.2 Rôle du paramètre lambda

**Question** : Quel rôle joue le paramètre lambda ? Comment choisir
lambda pour diminuer ou rétrécir (effet de shrinkage) les coefficients ?

**Réponse** :

Le paramètre $\lambda$ est le **paramètre de régularisation** qui
contrôle le compromis biais-variance :

$E(\tau_j | \lambda^2) = \frac{2}{\lambda^2} \quad \Rightarrow \quad E(V(\beta_j | \sigma^2_\epsilon, \lambda^2)) = \frac{2\sigma^2_\epsilon}{\lambda^2}$

**Effet de** $\lambda$ :

| Valeur de $\lambda$ | Effet sur les $\beta$ | Variance a priori | Interprétation |
|------------------|--------------------|-----------------|-----------------|
| $\lambda$ petit ($\to 0$) | Faible shrinkage | Grande | Proche des moindres carrés |
| $\lambda$ modéré | Shrinkage équilibré | Modérée | Compromis biais–variance |
| $\lambda$ grand ($\to \infty$) | Shrinkage fort | Petite | $\beta \to 0$ (sous‑ajustement) |

**Comment choisir** $\lambda$ pour augmenter le shrinkage ?

1.  **Augmenter** $\lambda$ directement $\to$ variance a priori diminue
    $\to$ coefficients rétrécis
2.  En pratique, on peut :
    -   Fixer des hyperparamètres $(e, f)$ de la loi Gamma pour
        favoriser des $\lambda$ élevés
    -   Utiliser la validation croisée pour optimiser $\lambda$
    -   Examiner les distributions a posteriori pour différentes valeurs

**Choix des hyperparamètres** : - Pour un shrinkage modéré :
$(e, f) \approx (1, 1)$ ou $(2, 2)$ - Pour un shrinkage fort : augmenter
$e$ (forme) favorise des $\lambda$ plus grands - Pour un a priori vague
: $(e, f) \approx (0.01, 0.01)$

## 3.3 Estimation du modèle

```{r lasso_bayesian_estimation}
# Hyperparamètres autour de 1 et 2
# Justification : 
# - (1,1) donne E[λ²] = e/f = 1, Var[λ²] = e/f² = 1 (assez vague)
# - (2,2) donne E[λ²] = 1, Var[λ²] = 0.5 (plus informatif)

nIter_lasso <- 12000
burnIn_lasso <- 2000

# Modèle 1 : hyperparamètres (1, 1)
lasso1_model <- BGLR(
  y = Y_train,
  ETA = list(list(X = X_train, model = "BL")),  # BL = Bayesian LASSO
  nIter = nIter_lasso,
  burnIn = burnIn_lasso,
  verbose = FALSE
)

# Modèle 2 : hyperparamètres (2, 2) - plus de shrinkage
# Note: BGLR n'expose pas directement les hyperparamètres de λ
# On utilisera le modèle par défaut et comparera avec glmnet pour validation

# Récupération des estimations (modèle 1)
mu_hat_lasso <- lasso1_model$mu
beta_hat_lasso <- lasso1_model$ETA[[1]]$b
tau_hat_lasso <- lasso1_model$ETA[[1]]$tau  # Paramètres tau_j
sigma2_e_lasso <- lasso1_model$varE

cat("Estimation de mu:", mu_hat_lasso, "\n")
cat("Variance résiduelle (sigma²_epsilon):", sigma2_e_lasso, "\n")
cat("\nStatistiques des τ estimés :\n")
summary(tau_hat_lasso)
```

**Pourquoi des hyperparamètres autour de 1 et 2 ?**

1.  **Échelle raisonnable** : Avec des données normalisées, on s'attend
    à ce que $\lambda^2 \sim 1$
2.  **A priori modérément informatif** :
    -   $(1,1)$ : distribution assez diffuse, laisse les données parler
    -   $(2,2)$ : un peu plus concentrée, favorise légèrement le
        shrinkage
3.  **Éviter les extrêmes** :
    -   Trop petit (\< 0.1) : a priori trop vague, convergence lente
    -   Trop grand (\> 10) : sur-régularisation, perte d'information

```{r lasso_beta_summary}
# Statistiques des coefficients LASSO
summary_beta_lasso <- data.frame(
  Statistique = c("Moyenne", "Écart-type", "Min", "Q1", "Médiane", "Q3", "Max", "Nb. = 0"),
  Valeur = c(
    mean(beta_hat_lasso),
    sd(beta_hat_lasso),
    min(beta_hat_lasso),
    quantile(beta_hat_lasso, 0.25),
    median(beta_hat_lasso),
    quantile(beta_hat_lasso, 0.75),
    max(beta_hat_lasso),
    sum(abs(beta_hat_lasso) < 1e-6)  # Coefficients quasi-nuls
  )
)

kable(summary_beta_lasso, digits = 4,
      caption = "Statistiques des coefficients LASSO bayésien")
```

```{r plot_lasso_beta}
# Visualisation des coefficients
beta_df_lasso <- data.frame(
  index = 1:length(beta_hat_lasso),
  beta = beta_hat_lasso,
  tau = tau_hat_lasso
)

p1 <- ggplot(beta_df_lasso, aes(x = index, y = beta)) +
  geom_point(alpha = 0.6, color = "darkgreen") +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Coefficients LASSO bayésien",
       x = "Indice de la variable",
       y = "Valeur du coefficient") +
  theme_minimal()

p2 <- ggplot(beta_df_lasso, aes(x = index, y = tau)) +
  geom_point(alpha = 0.6, color = "purple") +
  labs(title = expression("Paramètres" * tau * "(LASSO bayésien)"),
       x = "Indice de la variable",
       y = expression(tau)
      ) +
  theme_minimal()

gridExtra::grid.arrange(p1, p2, ncol = 1)
```

## 3.4 Comparaison distributions a posteriori vs a priori

**Question** : Comparez quelques distributions a posteriori à celles a
priori.

```{r prior_posterior_comparison}
# Sélection de 4 coefficients : 2 importants, 2 faibles
important_indices <- order(abs(beta_hat_lasso), decreasing = TRUE)[1:2]
weak_indices <- order(abs(beta_hat_lasso), decreasing = FALSE)[1:2]

# Pour la comparaison, on simule des échantillons de la loi a priori
# A priori : β_j ~ Laplace(0, scale)
# Avec E[τ_j] = 2/λ² et β_j|τ_j ~ N(0, σ²_ε * τ_j)

# Simulation a priori (approximation)
set.seed(789)
lambda_prior <- 1  # Valeur moyenne attendue
n_sim <- 1000

beta_prior_samples <- replicate(4, {
  tau_sim <- rexp(n_sim, rate = lambda_prior^2 / 2)
  rnorm(n_sim, mean = 0, sd = sqrt(sigma2_e_lasso * tau_sim))
})

# Pour a posteriori, on simulerait normalement les traces
# Ici, on approxime par une normale centrée sur l'estimation
beta_posterior_samples <- sapply(c(important_indices, weak_indices), function(idx) {
  rnorm(n_sim, mean = beta_hat_lasso[idx], sd = abs(beta_hat_lasso[idx]) * 0.2)
})

# Création du dataframe pour plotting
comparison_dist_df <- data.frame(
  value = c(as.vector(beta_prior_samples), as.vector(beta_posterior_samples)),
  distribution = rep(c(rep("A priori", n_sim * 4), rep("A posteriori", n_sim * 4))),
  coefficient = rep(rep(c("β_important1", "β_important2", "β_faible1", "β_faible2"), 
                        each = n_sim), 2)
)

# On remplace les labels Unicode par des expressions plotmat
comparison_dist_df$coefficient <- factor(
  comparison_dist_df$coefficient,
  levels = c("β_important1", "β_important2", "β_faible1", "β_faible2"),
  labels = c(
    "beta[important1]",
    "beta[important2]",
    "beta[faible1]",
    "beta[faible2]"
  )
)

ggplot(comparison_dist_df, aes(x = value, fill = distribution)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~coefficient, scales = "free", ncol = 2, labeller = label_parsed) +
  labs(
    title = "Comparaison distributions a priori vs a posteriori",
    subtitle = "LASSO bayésien : mise à jour bayésienne",
    x = "Valeur du coefficient",
    y = "Densité",
    fill = ""
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")
```

**Interprétation** :

Les graphiques révèlent la **mise à jour bayésienne** en action :

1. **Coefficients importants** (β_important1, β_important2) :

   - **A priori** : Distribution Laplace centrée en 0 avec pic prononcé (shrinkage fort attendu)
   - **A posteriori** : Distribution déplacée loin de zéro et fortement resserrée
   - **Signification** : Les données apportent une information **très forte** qui domine l'a priori. La parcimonie initiale est "vaincue" par l'évidence empirique.

2. **Coefficients faibles** (β_faible1, β_faible2) :

   - **A priori** : Même pic en 0
   - **A posteriori** : Distribution encore plus concentrée autour de 0, avec variance réduite
   - **Signification** : L'absence de signal dans les données **renforce** l'a priori parcimonieux. Le LASSO "confirme" que ces variables sont non pertinentes.

3. **Mécanisme du LASSO** :

   - Pour les grands coefficients : queues lourdes de la Laplace → faible pénalisation
   - Pour les petits coefficients : pic en 0 de la Laplace → shrinkage vers zéro exact
   - **Résultat** : Sélection automatique sans décision binaire arbitraire

Cette flexibilité explique pourquoi le LASSO bayésien surpasse Bayes A en parcimonie tout en maintenant une performance prédictive comparable (corrélation 0.893 vs 0.899).

## 3.5 Prédiction et comparaison

```{r lasso_prediction}
# Prédictions
Y_pred_lasso <- mu_hat_lasso + X_test %*% beta_hat_lasso

# Corrélation
cor_lasso <- cor(Y_test, Y_pred_lasso)

# Comparaison avec les méthodes précédentes
comparison_pred_df <- data.frame(
  Modèle = c("RR-BLUP", "Bayes A", "LASSO Bayésien"),
  Corrélation = c(cor_rr, cor_bayesA, cor_lasso),
  Rang = rank(-c(cor_rr, cor_bayesA, cor_lasso))
)

kable(comparison_pred_df, digits = 4,
      caption = "Comparaison des performances prédictives : 3 premières méthodes")
```

```{r plot_all_predictions}
# Graphique comparatif des 3 méthodes
all_preds <- rbind(
  data.frame(Observed = Y_test, Predicted = as.vector(Y_pred_rr), Model = "RR-BLUP"),
  data.frame(Observed = Y_test, Predicted = as.vector(Y_pred_bayesA), Model = "Bayes A"),
  data.frame(Observed = Y_test, Predicted = as.vector(Y_pred_lasso), Model = "LASSO Bayésien")
)

ggplot(all_preds, aes(x = Observed, y = Predicted, color = Model)) +
  geom_point(alpha = 0.6) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  facet_wrap(~Model) +
  labs(title = "Comparaison des prédictions : RR, Bayes A, LASSO",
       x = "Scores observés",
       y = "Scores prédits") +
  theme_minimal() +
  theme(legend.position = "none")
```

**Analyse comparative** :

-   **Meilleur modèle** :
    `r comparison_pred_df$Modèle[which.max(comparison_pred_df$Corrélation)]`
    avec une corrélation de
    `r round(max(comparison_pred_df$Corrélation), 3)`

-   **Interprétation** :

    -   Si LASSO \> Bayes A : la **parcimonie** est bénéfique (beaucoup
        de variables inutiles)
    -   Si Bayes A $\approx$ LASSO : les deux approches capturent
        l'information de manière similaire
    -   Les écarts peuvent être faibles car toutes les méthodes
        régularisent

## 3.6 et 3.7 Sélection de variables et comparaison

```{r lasso_variable_selection}
# Sélection basée sur |β| avec seuil adaptatif
# Critère : boxplot comme pour RR
Q1_lasso <- quantile(abs(beta_hat_lasso), 0.25)
Q3_lasso <- quantile(abs(beta_hat_lasso), 0.75)
IQR_lasso <- Q3_lasso - Q1_lasso
seuil_lasso <- Q3_lasso + 1.5 * IQR_lasso

selected_vars_lasso <- which(abs(beta_hat_lasso) > seuil_lasso)
n_selected_lasso <- length(selected_vars_lasso)

cat("Nombre de variables sélectionnées (LASSO):", n_selected_lasso, "\n")
cat("Nombre de coefficients quasi-nuls:", sum(abs(beta_hat_lasso) < 1e-4), "\n")
```

```{r compare_all_selections}
# Top 10 de chaque méthode
top_10_lasso <- order(abs(beta_hat_lasso), decreasing = TRUE)[1:10]

# Variables communes 2 à 2
common_rr_bayesA <- intersect(top_10_rr, top_10_bayesA)
common_rr_lasso <- intersect(top_10_rr, top_10_lasso)
common_bayesA_lasso <- intersect(top_10_bayesA, top_10_lasso)
common_all <- Reduce(intersect, list(top_10_rr, top_10_bayesA, top_10_lasso))

comparison_selection <- data.frame(
  Comparaison = c("RR ∩ Bayes A", "RR ∩ LASSO", "Bayes A ∩ LASSO", "RR ∩ Bayes A ∩ LASSO"),
  Nb_variables_communes = c(
    length(common_rr_bayesA),
    length(common_rr_lasso),
    length(common_bayesA_lasso),
    length(common_all)
  )
)

kable(comparison_selection,
      caption = "Nombre de variables communes dans le top 10")
```

```{r top10_comparison_table}
# Tableau détaillé du top 10 LASSO avec comparaison
top10_detailed <- data.frame(
  Variable = X_cols[top_10_lasso],
  Beta_LASSO = beta_hat_lasso[top_10_lasso],
  Beta_BayesA = beta_hat_bayesA[top_10_lasso],
  Beta_RR = beta_hat_rr[top_10_lasso],
  In_BayesA_Top10 = top_10_lasso %in% top_10_bayesA,
  In_RR_Top10 = top_10_lasso %in% top_10_rr
)

kable(top10_detailed, digits = 4,
      caption = "Top 10 LASSO : comparaison des coefficients") %>%
  kable_styling(latex_options = c("hold_position", "scale_down"))
```

```{r plot_selection_comparison}
# Diagramme de Venn conceptuel (counts)
library(ggplot2)

venn_data <- data.frame(
  Méthode = c("RR-BLUP", "Bayes A", "LASSO", "RR ∩ Bayes A", "RR ∩ LASSO", 
              "Bayes A ∩ LASSO", "Tous"),
  Count = c(
    10 - length(common_rr_bayesA) - length(common_rr_lasso) + length(common_all),
    10 - length(common_rr_bayesA) - length(common_bayesA_lasso) + length(common_all),
    10 - length(common_rr_lasso) - length(common_bayesA_lasso) + length(common_all),
    length(common_rr_bayesA) - length(common_all),
    length(common_rr_lasso) - length(common_all),
    length(common_bayesA_lasso) - length(common_all),
    length(common_all)
  )
)

ggplot(venn_data, aes(x = Méthode, y = Count, fill = Méthode)) +
  geom_col() +
  geom_text(aes(label = Count), vjust = -0.5) +
  labs(title = "Recoupement des top 10 entre méthodes",
       y = "Nombre de variables") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "none")
```

**Synthèse de la comparaison** :

1.  **Cohérence globale** : `r length(common_all)` variables sont dans
    le top 10 des **trois méthodes**, indiquant un signal robuste

2.  **Spécificités du LASSO** :

    -   Sélectionne `r n_selected_lasso` variables au total (vs
        `r n_selected_bayesA` pour Bayes A)
    -   Force `r sum(abs(beta_hat_lasso) < 1e-4)` coefficients à zéro
        exact (parcimonie)

3.  **Variables consensus** :

    ```{r consensus_vars}
    if(length(common_all) > 0) {
      cat("Variables dans le top 10 des 3 méthodes :\n")
      print(X_cols[common_all])
    }
    ```

## 3.8 Explication des différences LASSO vs Bayes A

**Question** : Qu'est-ce qui pourrait expliquer la différence entre les
variables retenues par le LASSO Bayésien et celles retenues par Bayes A
(ridge bayésien) ?

**Réponse** :

Les différences s'expliquent par la **nature fondamentale des
pénalisations** :

### 1. Géométrie de la régularisation

```{r geometry_illustration, echo=FALSE}
# Illustration conceptuelle : régions de contrainte L1 vs L2

# Grille fine pour éviter les trous
theta_grid <- expand.grid(
  beta1 = seq(-2, 2, length.out = 400),
  beta2 = seq(-2, 2, length.out = 400)
)

# Normes L1 et L2
theta_grid$L1 <- abs(theta_grid$beta1) + abs(theta_grid$beta2)
theta_grid$L2 <- theta_grid$beta1^2 + theta_grid$beta2^2

# Extraction des points proches du niveau 1
eps <- 0.03  # bande plus large pour garantir des points

contour_df <- rbind(
  data.frame(
    beta1 = theta_grid$beta1[abs(theta_grid$L1 - 1) < eps],
    beta2 = theta_grid$beta2[abs(theta_grid$L1 - 1) < eps],
    Type = "L1 (LASSO)"
  ),
  data.frame(
    beta1 = theta_grid$beta1[abs(theta_grid$L2 - 1) < eps],
    beta2 = theta_grid$beta2[abs(theta_grid$L2 - 1) < eps],
    Type = "L2 (Ridge)"
  )
)

# Plot
ggplot(contour_df, aes(x = beta1, y = beta2, color = Type)) +
  geom_path(size = 1.2) +
  geom_vline(xintercept = 0, linetype = "dotted", alpha = 0.5) +
  geom_hline(yintercept = 0, linetype = "dotted", alpha = 0.5) +
  geom_point(aes(x = 0, y = 0), size = 3, color = "red") +
  
  # Annotations compatibles LaTeX
  annotate(
    "text", x = 1.2, y = 0,
    label = "Solution~LASSO~(beta[2] == 0)",
    color = "darkgreen", hjust = 0, parse = TRUE
  ) +
  annotate(
    "text", x = 0.7, y = 0.7,
    label = "Solution~Ridge~(beta[1] != 0 ~','~ beta[2] != 0)",
    color = "blue", hjust = 0, parse = TRUE
  ) +
  
  coord_fixed() +
  labs(
    title = "Géométrie des contraintes : L1 vs L2",
    subtitle = "Le LASSO favorise les solutions sur les axes (sparsité)",
    x = expression(beta[1]),
    y = expression(beta[2]),
    color = ""
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")
```

**Géométrie** : - **L1 (LASSO)** : Contrainte en forme de **losange**
$\to$ solutions aux **sommets** (axes) $\to$ certains $\beta = 0$ - **L2
(Ridge)** : Contrainte **circulaire** $\to$ solutions rarement sur les
axes $\to$ tous $\beta \neq 0$

### 2. Corrélation entre variables

Lorsque deux variables sont **fortement corrélées** :

-   **Ridge (Bayes A)** :
    -   Répartit le poids entre les deux variables corrélées
    -   Coefficients de taille similaire pour variables similaires
    -   **Stabilité** face à la multicolinéarité
-   **LASSO** :
    -   Sélectionne arbitrairement **une seule** des variables corrélées
    -   Met l'autre à zéro
    -   **Instabilité** si les variables sont échangeables

```{r correlation_effect}
# Analyse de corrélation pour expliquer les différences
# Calcul de la matrice de corrélation des variables sélectionnées

# Variables uniquement dans Bayes A (pas dans LASSO)
only_bayesA <- setdiff(top_10_bayesA, top_10_lasso)
# Variables uniquement dans LASSO (pas dans Bayes A)
only_lasso <- setdiff(top_10_lasso, top_10_bayesA)

if(length(only_bayesA) > 0 & length(only_lasso) > 0) {
  cat("Variables uniquement retenues par Bayes A (pas LASSO):\n")
  print(X_cols[only_bayesA])
  cat("\nVariables uniquement retenues par LASSO (pas Bayes A):\n")
  print(X_cols[only_lasso])
  
  # Analyse de corrélation entre ces groupes
  if(length(only_bayesA) > 0 & length(only_lasso) > 0) {
    cor_cross <- cor(X_train[, only_bayesA], X_train[, only_lasso])
    cat("\nCorrélations entre variables divergentes:\n")
    print(round(cor_cross, 3))
    
    if(max(abs(cor_cross)) > 0.7) {
      cat("\n⚠ Forte corrélation détectée (>0.7) : cela explique les choix différents\n")
    }
  }
}
```

### 3. Force du signal

| Situation | Bayes A (Ridge) | LASSO |
|----------------------|---------------------------------|-----------------|
| **Signal fort** | Coefficient modéré (shrinkage proportionnel) | Coefficient préservé (peu de shrinkage) |
| **Signal faible** | Coefficient faible mais non nul | Coefficient = 0 (élimination) |
| **Signal moyen** | Coefficient réduit | Peut être éliminé ou préservé |

### 4. Nombre de variables pertinentes

-   **Si** $p \gg n$ (beaucoup de variables, peu d'observations) :
    -   **LASSO** : Sélectionne au maximum n variables $\to$ parcimonie
        forcée
    -   **Bayes A** : Conserve toutes les variables avec shrinkage
-   **Si signal distribué sur beaucoup de variables** :
    -   **Bayes A** : Capture mieux les effets faibles cumulés
    -   **LASSO** : Peut manquer des variables avec petits effets

### 5. Synthèse des différences observées

```{r synthesis_differences}
# Comparaison quantitative des méthodes
comparison_methods <- data.frame(
  Critère = c(
    "Nb. variables retenues (top 10)",
    "Nb. coefficients quasi-nuls",
    "Magnitude moyenne $\\lvert \\beta \\rvert$",
    "Écart-type des $\\lvert \\beta \\rvert$",
    "Corrélation prédictive"
  ),
  Bayes_A = c(
    length(top_10_bayesA),
    sum(abs(beta_hat_bayesA) < 1e-6),
    mean(abs(beta_hat_bayesA)),
    sd(abs(beta_hat_bayesA)),
    cor_bayesA
  ),
  LASSO = c(
    length(top_10_lasso),
    sum(abs(beta_hat_lasso) < 1e-6),
    mean(abs(beta_hat_lasso)),
    sd(abs(beta_hat_lasso)),
    cor_lasso
  )
)

comparison_methods$Différence <- comparison_methods$LASSO - comparison_methods$Bayes_A

kable(comparison_methods, digits = 4,
      escape = FALSE,   # IMPORTANT pour afficher le LaTeX dans la colonne Critère
      caption = "Comparaison quantitative : Bayes A vs LASSO bayésien") %>%
  kable_styling(latex_options = "hold_position")
```

**Analyse du tableau 15** :

La comparaison quantitative révèle des différences subtiles mais importantes :

1. **Magnitude moyenne $\mid \beta \mid$** : LASSO (0.222) < Bayes A (0.228)
   - Le LASSO produit des coefficients légèrement plus petits en moyenne
   - Signe d'un **shrinkage global plus prononcé**

2. **Écart-type des $\mid \beta \mid$** : LASSO (0.351) < Bayes A (0.376)
   - Les coefficients LASSO sont plus homogènes
   - La pénalisation L1 "compresse" la distribution

3. **Coefficients quasi-nuls** : 8 pour chaque méthode
   - Nombre similaire, mais identités différentes
   - Le LASSO choisit différemment parmi les variables corrélées

4. **Corrélation prédictive** : quasi-identique (différence de 0.5%)
   - Les deux méthodes capturent l'information essentielle
   - Le choix dépend de l'objectif : parcimonie (LASSO) vs stabilité (Bayes A)

**Recommandation** : Dans notre contexte où l'interprétabilité est primordiale (recommandations à la chaîne câblée), le LASSO bayésien est préférable car il identifie plus clairement les leviers d'action.

### Conclusion sur les différences

Les variables retenues diffèrent principalement à cause de :

1.  **Philosophie de régularisation** : Ridge préserve, LASSO élimine
2.  **Gestion de la corrélation** : Ridge partage, LASSO choisit
3.  **Compromis biais-variance** : Ridge privilégie la variance, LASSO
    privilégie le biais
4.  **Objectif** :
    -   Bayes A $\to$ meilleure **prédiction** (conserver information)
    -   LASSO $\to$ meilleure **interprétation** (modèle parcimonieux)

Dans notre contexte, si LASSO performe mieux, cela suggère que **peu de
chaînes** influencent réellement la satisfaction, et que de nombreuses
variables sont du **bruit**.

\newpage

# 4 Stochastic Search Variable Selection (SSVS)

## Rappel théorique

La méthode **SSVS** (George & McCulloch, 1993) introduit un vecteur binaire $\gamma = (\gamma_1, \ldots, \gamma_p)$ :

$$
\gamma_j = \begin{cases}
1 & \text{si } \beta_j \neq 0 \text{ (sélection)} \\
0 & \text{si } \beta_j = 0 \text{ (non sélection)}
\end{cases}
$$

Le modèle complet :

$$
\begin{cases}
Y = \mu \mathbf{1} + X\beta + \epsilon \\
\epsilon \sim \mathcal{N}(0, \sigma^2_\epsilon I) \\
\beta_\gamma | \gamma, \sigma^2_\epsilon \sim \mathcal{N}(0, \sigma^2_\epsilon c (X'_\gamma X_\gamma)^{-1}) \\
P(\gamma_j = 1) = \pi \\
f(\sigma^2_\epsilon) = 1/\sigma^2_\epsilon \\
\mu \sim \text{Uniform}
\end{cases}
$$

**Paramètres clés** :
- $c > 0$ : **coefficient de sélection** (typiquement 10-100)
- $\pi \in (0,1)$ : **probabilité a priori** de sélectionner une variable
- $d_\gamma = \sum \gamma_j$ : nombre de variables sélectionnées

**A priori de Zellner** : La structure $c(X'_\gamma X_\gamma)^{-1}$ adapte automatiquement la covariance à la structure des données.

## 4.1 Algorithme d'estimation

**Question** : Est-ce qu'on utilise un Metropolis-Hasting ou un Gibbs sampler dans notre approche SSVS ? Quelle est la loi (appelée « proposal ») qui choisit le nouveau sous-ensemble de variables ?

**Réponse** :

### Type d'algorithme : Metropolis-Hastings

On utilise un **algorithme de Metropolis-Hastings** car :

1. **La loi de $\gamma|Y$ n'est pas standard** : On ne peut pas l'échantillonner directement
2. **On peut calculer sa densité** (à une constante près) :

$$
f(\gamma|Y) \propto (1+c)^{d_\gamma/2} \prod_{j=1}^p \pi^{\gamma_j}(1-\pi)^{1-\gamma_j} \times \left[\frac{1}{2}\tilde{Y}'\left(I - \frac{c}{1+c}X_\gamma(X'_\gamma X_\gamma)^{-1}X'_\gamma\right)\tilde{Y}\right]^{-\frac{n-1}{2}}
$$

où $\tilde{Y} = Y - \bar{Y}\mathbf{1}$ (observations centrées).

### Proposition (proposal)

La **proposition** est un **noyau symétrique** de type "naissance-mort" :

**À l'itération $t$** :
1. Choisir aléatoirement $r$ positions dans $\gamma^{(t)}$
2. Inverser ces positions : $1 \to 0$ et $0 \to 1$
3. Obtenir $\gamma^*$ candidat

**Propriété** : $q(\gamma^*|\gamma^{(t)}) = q(\gamma^{(t)}|\gamma^*)$ (symétrie)

**Algorithme complet** :

À l'étape t+1 :

1. Générer $\gamma^{*} \sim q(\cdot \mid \gamma^{(t)})$ [changer $r$ éléments aléatoirement)]

2. Calculer $\rho = \min\left\{1, \frac{f(\gamma^{*}\mid Y)}{f(\gamma^{(t)}\mid Y)}\right\}$

3. Accepter $\gamma^{(t+1)} = \begin{cases} \gamma^{*} & \text{avec probabilité } \rho,\\ \gamma^{(t)} & \text{sinon} \end{cases}$

**Simplification** : Comme $q$ est symétrique, le ratio de Metropolis se réduit à :

$$
\rho(\gamma^*, \gamma^{(t)}) = \min\left\{1, \frac{f(\gamma^*|Y)}{f(\gamma^{(t)}|Y)}\right\}
$$

**Avantages de SSVS** :
- Très rapide : on manipule seulement $X_\gamma$ (sous-matrice)
- Gère haute dimension (génomique, $p \gg n$)
- Donne une **distribution de probabilité** sur les modèles

## 4.2 Sélection de variables

```{r ssvs_function}
# Fonction SSVS (à adapter selon votre code TP)
# Ici on donne la structure générale

selection_SSVS <- function(Y, X, nIter = 10000, burnIn = 2000, 
                          pi_prior = 0.1, c_param = 100, r_change = 3) {
  n <- length(Y)
  p <- ncol(X)
  
  # Centrage des données
  Y_centered <- Y - mean(Y)
  
  # Initialisation de gamma (10% de variables sélectionnées)
  gamma <- rep(0, p)
  gamma[sample(1:p, size = floor(p * pi_prior))] <- 1
  
  # Stockage des gamma
  gamma_samples <- matrix(0, nrow = nIter - burnIn, ncol = p)
  
  # Fonction de log-densité (évite les overflow)
  log_posterior_gamma <- function(gamma_vec) {
    d_gamma <- sum(gamma_vec)
    
    if(d_gamma == 0) return(-Inf)  # Pas de variables
    
    #if (d_gamma > n - 1) return(-Inf)  # Trop de variables sélectionnées
    
    # Indices des variables sélectionnées
    selected <- which(gamma_vec == 1)
    X_gamma <- X[, selected, drop = FALSE]
    
    # Vérification d'inversibilité : Vérification du rang (plus fiable que det)
    qr_X <- qr(X_gamma)
    if (qr_X$rank < d_gamma) {
      return(-Inf)
    }
    
    # Calcul de XtX
    
    #XtX <- crossprod(X_gamma)
    
    # Ajout d'une petite régularisation pour la stabilité numérique
    lambda_ridge <- 1e-3
    XtX <- crossprod(X_gamma) + diag(lambda_ridge, d_gamma)

    
    # Calcul du terme principal
    
    # Option 1 : résolution stable via solve(A, B)
    # P_gamma <- X_gamma %*% solve(XtX, t(X_gamma))
    
    # Option 2 (souvent plus stable) : qr.solve
    P_gamma <- X_gamma %*% qr.solve(XtX, t(X_gamma))
    
    RSS <- sum(Y_centered^2) - c_param/(1+c_param) * t(Y_centered) %*% P_gamma %*% Y_centered
    
    # Log-densité
    log_dens <- (d_gamma/2) * log(1 + c_param) + 
                 sum(gamma_vec * log(pi_prior) + (1 - gamma_vec) * log(1 - pi_prior)) -
                 ((n-1)/2) * log(RSS)
    
    return(as.numeric(log_dens))
  }
  
  # Metropolis-Hastings
  for(iter in 1:nIter) {
    # Proposition : changer r éléments aléatoirement
    gamma_star <- gamma
    positions <- sample(1:p, size = r_change)
    gamma_star[positions] <- 1 - gamma_star[positions]
    
    # Ratio d'acceptation (log-échelle pour stabilité)
    log_alpha <- log_posterior_gamma(gamma_star) - log_posterior_gamma(gamma)
    
    # Acceptation
    if(log(runif(1)) < log_alpha) {
      gamma <- gamma_star
    }
    
    # Stockage après burn-in
    if(iter > burnIn) {
      gamma_samples[iter - burnIn, ] <- gamma
    }
    
    # Affichage progression
    if(iter %% 1000 == 0) cat("Itération:", iter, "- Variables:", sum(gamma), "\n")
  }
  
  # Fréquences de sélection
  selection_freq <- colMeans(gamma_samples)
  
  return(list(
    gamma_samples = gamma_samples,
    selection_freq = selection_freq,
    final_gamma = gamma
  ))
}
```

```{r ssvs_estimation}
# Application de SSVS
set.seed(2026)

# Paramètres
pi_prior <- 0.1   # A priori : 10% des variables sélectionnées
c_param <- 50     # Coefficient de sélection modéré
nIter_ssvs <- 15000  # Nombre d'itérations total
burnIn_ssvs <- 3000   # Nombre d'itérations de burn-in

cat("Lancement de SSVS...\n")
cat("Paramètres : $\\pi=$", pi_prior, ", c =", c_param, "\n\n")

ssvs_result <- selection_SSVS(
  Y = Y_train,
  X = X_train,
  nIter = nIter_ssvs,
  burnIn = burnIn_ssvs,
  pi_prior = pi_prior,
  c_param = c_param,
  r_change = 3
)

# Fréquences de sélection
selection_freq_ssvs <- ssvs_result$selection_freq
```

```{r ssvs_results}
# Statistiques de sélection
summary_freq <- summary(selection_freq_ssvs)
cat("Statistiques des fréquences de sélection :\n")
print(summary_freq)

# Nombre de variables fréquemment sélectionnées (> 50%)
n_selected_ssvs <- sum(selection_freq_ssvs > 0.5)
cat("\nNombre de variables sélectionnées (fréquence > 50%):", n_selected_ssvs, "\n")

# Top 10 variables
top_10_ssvs <- order(selection_freq_ssvs, decreasing = TRUE)[1:10]
top_ssvs_df <- data.frame(
  Variable = X_cols[top_10_ssvs],
  Frequence_Selection = selection_freq_ssvs[top_10_ssvs]
)

kable(top_ssvs_df, digits = 3,
      caption = "Top 10 des variables par fréquence de sélection (SSVS)")
```

```{r plot_ssvs_frequencies}
# Graphique des fréquences
freq_df_ssvs <- data.frame(
  index = 1:length(selection_freq_ssvs),
  frequency = selection_freq_ssvs,
  selected = selection_freq_ssvs > 0.5
)

ggplot(freq_df_ssvs, aes(x = index, y = frequency, color = selected)) +
  geom_point(alpha = 0.6) +
  geom_hline(yintercept = 0.5, linetype = "dashed", color = "red") +
  scale_color_manual(values = c("FALSE" = "gray60", "TRUE" = "darkgreen"),
                     labels = c("< 50%", "≥ 50%")) +
  labs(title = "Fréquences de sélection SSVS",
       subtitle = paste(n_selected_ssvs, "variables sélectionnées (fréquence ≥ 50%)"),
       x = "Indice de la variable",
       y = "Fréquence de sélection",
       color = "Sélection") +
  theme_minimal() +
  theme(legend.position = "bottom")
```

```{r ssvs_trace_example}
# Trace de quelques gamma (évolution de la sélection)
# Sélection de 4 variables : 2 souvent sélectionnées, 2 rarement

often_selected <- top_10_ssvs[1:2]
rarely_selected <- order(selection_freq_ssvs)[1:2]

trace_indices <- c(often_selected, rarely_selected)
n_samples <- nrow(ssvs_result$gamma_samples)

trace_df <- data.frame(
  iteration = rep(1:n_samples, 4),
  gamma = c(ssvs_result$gamma_samples[, trace_indices[1]],
            ssvs_result$gamma_samples[, trace_indices[2]],
            ssvs_result$gamma_samples[, trace_indices[3]],
            ssvs_result$gamma_samples[, trace_indices[4]]),
  variable = rep(paste0("Var_", trace_indices, " (", 
                        X_cols[trace_indices], ")"), each = n_samples),
  type = rep(c("Souvent sélectionnée", "Souvent sélectionnée",
               "Rarement sélectionnée", "Rarement sélectionnée"), each = n_samples)
)

ggplot(trace_df, aes(x = iteration, y = gamma, color = type)) +
  geom_line(alpha = 0.7) +
  facet_wrap(~variable, ncol = 2) +
  labs(title = expression("Traces de sélection (" * gamma * ") au cours des itérations"),
       subtitle = "1 = sélectionnée, 0 = non sélectionnée",
       x = "Itération (post burn-in)",
       y = expression(gamma),
       color = "") +
  theme_minimal() +
  theme(legend.position = "bottom")
```

**Interprétation des traces** :

- **Variables souvent sélectionnées** : $\gamma = 1$ la plupart du temps (signal fort)
- **Variables rarement sélectionnées** : $\gamma = 0$ la plupart du temps (non pertinentes)
- **Alternances** : Indicateur d'incertitude (variables borderline)

## 4.3 Influence des hyperparamètres

**Question** : Faites varier quelques hyper-paramètres pour voir l'influence sur la sélection.

```{r ssvs_sensitivity}
# Test de sensibilité : variation de π et c

# Configuration 1 : π faible (a priori parcimonieux)
pi1 <- 0.05
ssvs1 <- selection_SSVS(Y_train, X_train, nIter = 8000, burnIn = 2000,
                        pi_prior = pi1, c_param = 50, r_change = 3)

# Configuration 2 : π élevé (a priori généreux)
pi2 <- 0.3
ssvs2 <- selection_SSVS(Y_train, X_train, nIter = 8000, burnIn = 2000,
                        pi_prior = pi2, c_param = 50, r_change = 3)

# Configuration 3 : c faible (peu de confiance dans les données)
ssvs3 <- selection_SSVS(Y_train, X_train, nIter = 8000, burnIn = 2000,
                        pi_prior = 0.1, c_param = 10, r_change = 3)

# Configuration 4 : c élevé (forte confiance dans les données)
ssvs4 <- selection_SSVS(Y_train, X_train, nIter = 8000, burnIn = 2000,
                        pi_prior = 0.1, c_param = 200, r_change = 3)

# Comparaison des sélections
comparison_hyper <- data.frame(
  Configuration = c(
    "Référence (pi=0.1, c=50)",
    "π faible (pi=0.05, c=50)",
    "π élevé (pi=0.3, c=50)",
    "c faible (pi=0.1, c=10)",
    "c élevé (pi=0.1, c=200)"
  ),
  Nb_variables_50pct = c(
    sum(selection_freq_ssvs > 0.5),
    sum(ssvs1$selection_freq > 0.5),
    sum(ssvs2$selection_freq > 0.5),
    sum(ssvs3$selection_freq > 0.5),
    sum(ssvs4$selection_freq > 0.5)
  ),
  Nb_variables_80pct = c(
    sum(selection_freq_ssvs > 0.8),
    sum(ssvs1$selection_freq > 0.8),
    sum(ssvs2$selection_freq > 0.8),
    sum(ssvs3$selection_freq > 0.8),
    sum(ssvs4$selection_freq > 0.8)
  )
)

kable(comparison_hyper,
      caption = "Influence des hyperparamètres sur la sélection SSVS", escape = FALSE) %>%
  kable_styling(latex_options = "hold_position")
```

```{r plot_sensitivity}
# Visualisation comparative
sensitivity_df <- data.frame(
  Frequency = c(selection_freq_ssvs, ssvs1$selection_freq, ssvs2$selection_freq),
  Config = rep(c("π=0.1 (référence)", "π=0.05 (strict)", "π=0.3 (généreux)"),
               each = length(selection_freq_ssvs))
)

ggplot(sensitivity_df, aes(x = Frequency, fill = Config)) +
  geom_histogram(bins = 30, alpha = 0.7, position = "identity") +
  facet_wrap(~Config, ncol = 1) +
  geom_vline(xintercept = 0.5, linetype = "dashed", color = "red") +
  labs(title = "Distribution des fréquences de sélection selon π",
       x = "Fréquence de sélection",
       y = "Nombre de variables",
       fill = "") +
  theme_minimal() +
  theme(legend.position = "none")
```

**Analyse de l'influence des hyperparamètres** :

### Effet de $\pi$ (probabilité a priori)

| $\pi$ | Interprétation | Nb. variables sélectionnées | Usage |
|---|----------------|------------------------------|-------|
| **Faible** (0.05) | A priori très parcimonieux | Peu (seuil élevé) | Quand on veut un modèle très simple |
| **Modéré** (0.1) | Équilibre | Modéré | Cas général |
| **Élevé** (0.3) | A priori inclusif | Beaucoup | Exploration, éviter faux négatifs |

**Recommandation** : 
- Si incertitude sur la vraie dimensionalité : prendre $\pi$ modéré (0.1-0.2)
- Si on sait qu'il y a peu de vraies variables : $\pi$ faible (0.05)

### Effet de c (coefficient de sélection) c

| c | Interprétation | Effet | Usage |
|---|----------------|-------|-------|
| **Faible** (10) | Faible confiance données | Sélection conservatrice | Données bruitées |
| **Modéré** (50-100) | Confiance raisonnable | Équilibre | Cas général |
| **Élevé** (200+) | Forte confiance données | Sélection agressive | Données fiables, n grand |

Cependant, un $c$ trop grand peut causer des problèmes numériques (instabilité de $(X'_\gamma X_\gamma)^{-1}$)

**Choix optimal dans notre cas** :
- $\pi = 0.1$ : cohérent avec l'idée que ~10-20 chaînes sur 160 sont vraiment influentes
- $c = 50-100$ : compromis entre précision et stabilité

## Conclusion SSVS

SSVS offre une approche probabiliste de la sélection de variables :
- Fournit des **fréquences de sélection** plutôt qu'un modèle unique
- Permet de quantifier l'**incertitude** sur la sélection
- Très efficace en haute dimension grâce à la manipulation de $X_\gamma$ uniquement

\newpage

# 5 Comparaisons et Synthèse

Cette section synthétise les résultats des quatre méthodes bayésiennes et les compare aux approches classiques.

## 5.1 Classification des méthodes

**Question** : Classez les quatre approches précédentes en deux grands groupes de méthodes.

**Réponse** :

Les quatre méthodes se classent en **deux groupes fondamentaux** :

### Groupe 1 : Méthodes de SHRINKAGE (Régularisation continue)

| Méthode | Type de pénalité | Sélection exacte ? | Philosophie |
|---------|------------------|-------------------|-------------|
| **RR-BLUP** | Aucune (variance commune) | Non | Régression ridge la plus simple |
| **Bayes A** | L2 (quadratique) | Non | Ridge bayésien avec variances individuelles |
| **LASSO Bayésien** | L1 (absolue) | Oui (soft) | Parcimonie via shrinkage fort |

**Caractéristiques communes** :
- Tous les coefficients sont estimés simultanément
- Réduction continue des coefficients vers zéro
- Pas de décision binaire "in/out" (sauf LASSO qui peut mettre à 0)
- Estimation via MCMC (Gibbs sampler)

### Groupe 2 : Méthodes de SÉLECTION (Choix discret de variables)

| Méthode | Approche | Sélection exacte ? | Philosophie |
|---------|----------|-------------------|-------------|
| **SSVS** | Recherche stochastique | Oui (hard) | Exploration de l'espace des modèles |

**Caractéristiques** :
- Variable latente binaire $\gamma$ (in/out)
- Décision discrète sur chaque variable
- Fournit une **distribution de probabilité** sur les modèles
- Estimation via Metropolis-Hastings

### Visualisation de la classification

```{r method_classification}
# Tableau de synthèse
classification_df <- data.frame(
  Méthode = c("RR-BLUP", "Bayes A", "LASSO Bayésien", "SSVS"),
  Groupe = c("Shrinkage", "Shrinkage", "Shrinkage (+ sélection soft)", "Sélection"),
  Type_pénalité = c("Aucune", "L2 (Ridge)", "L1 (LASSO)", "Indicatrice"),
  Sélection_exacte = c("Non", "Non", "Partielle", "Oui"),
  Algorithme = c("REML", "Gibbs", "Gibbs", "Metropolis-Hastings"),
  Hyperparamètres = c(
    "0",
    "$\\sigma^2_{\\beta_j}$ (p)",
    "$\\lambda^2$, $\\tau_j$ (p+1)",
    "$\\pi$, c, $\\gamma$"
  )
)

kable(classification_df,
      caption = "Classification des quatre méthodes bayésiennes",
      escape = FALSE) %>%   # IMPORTANT pour rendre le LaTeX
  kable_styling(latex_options = c("hold_position", "scale_down")) %>%
  column_spec(2, bold = TRUE)
```

**Analogie avec les méthodes fréquentistes** :

```{r frequentist_analogy}
analogy_df <- data.frame(
  Méthode_Bayésienne = c("RR-BLUP", "Bayes A", "LASSO Bayésien", "SSVS"),
  Équivalent_Fréquentiste = c(
    "Régression Ridge simple",
    "Ridge avec pénalités adaptatives",
    "LASSO pénalisé",
    "Stepwise / Best Subset Selection"
  ),
  Avantage_Bayésien = c(
    "Quantification d'incertitude",
    "Pénalités données-dépendantes",
    "Distribution complète des $\\beta$",
    "Probabilités de sélection"
  )
)

kable(analogy_df,
      caption = "Analogies avec les méthodes fréquentistes") %>%
  kable_styling(latex_options = "hold_position")
```

## 5.2 Comparaison de l'effet de shrinkage

**Question** : En examinant les résultats obtenus avec RR-BLUP, Bayes A et LASSO bayésien, y a-t-il une méthode où l'effet « shrinkage » est plus prononcé ?

```{r shrinkage_comparison}
# Compilation des coefficients des 3 méthodes
shrinkage_df <- data.frame(
  Index = 1:length(beta_hat_rr),
  RR_BLUP = beta_hat_rr,
  Bayes_A = beta_hat_bayesA,
  LASSO = beta_hat_lasso
)

# Statistiques de shrinkage
shrinkage_stats <- data.frame(
  Méthode = c("RR-BLUP", "Bayes A", "LASSO Bayésien"),
  Moyenne_abs_beta = c(
    mean(abs(beta_hat_rr)),
    mean(abs(beta_hat_bayesA)),
    mean(abs(beta_hat_lasso))
  ),
  Mediane_abs_beta = c(
    median(abs(beta_hat_rr)),
    median(abs(beta_hat_bayesA)),
    median(abs(beta_hat_lasso))
  ),
  Ecart_type = c(
    sd(beta_hat_rr),
    sd(beta_hat_bayesA),
    sd(beta_hat_lasso)
  ),
  Max_abs_beta = c(
    max(abs(beta_hat_rr)),
    max(abs(beta_hat_bayesA)),
    max(abs(beta_hat_lasso))
  ),
  Nb_quasi_nuls = c(
    sum(abs(beta_hat_rr) < 0.01),
    sum(abs(beta_hat_bayesA) < 0.01),
    sum(abs(beta_hat_lasso) < 0.01)
  )
)

shrinkage_stats$Rang_Shrinkage <- rank(shrinkage_stats$Moyenne_abs_beta)

kable(shrinkage_stats, digits = 4,
      caption = "Comparaison quantitative du shrinkage") %>%
  kable_styling(latex_options = "hold_position")
```

```{r plot_shrinkage_comparison}
# Boxplot comparatif
shrinkage_long <- data.frame(
  beta = c(beta_hat_rr, beta_hat_bayesA, beta_hat_lasso),
  Méthode = rep(c("RR-BLUP", "Bayes A", "LASSO"), each = length(beta_hat_rr))
)

p1 <- ggplot(shrinkage_long, aes(x = Méthode, y = abs(beta), fill = Méthode)) +
  geom_boxplot() +
  scale_y_log10() +
  labs(title = "Distribution des coefficients (valeur absolue)",
       subtitle = "Échelle logarithmique",
       y = expression("|" * beta * "|")
      )+
  theme_minimal() +
  theme(legend.position = "none")

# Densité des coefficients
p2 <- ggplot(shrinkage_long, aes(x = beta, fill = Méthode)) +
  geom_density(alpha = 0.5) +
  xlim(c(-0.5, 0.5)) +
  labs(title = "Densité des coefficients (zoom sur [-0.5, 0.5])",
       x = expression(beta), y = "Densité") +
  theme_minimal() +
  theme(legend.position = "bottom")

gridExtra::grid.arrange(p1, p2, ncol = 1)
```

```{r shrinkage_by_magnitude}
# Analyse par catégorie de magnitude (grands, moyens, petits coefficients)
# Référence : coefficients OLS (sans pénalité) - simulation
ols_model <- lm(Y_train ~ X_train - 1)  # Sans intercept car déjà dans mu
beta_ols <- coef(ols_model)

# Catégorisation selon |β_OLS|
categories <- cut(abs(beta_ols), 
                  breaks = c(0, 0.05, 0.15, Inf),
                  labels = c("Petit", "Moyen", "Grand"))

shrinkage_by_category <- data.frame(
  Catégorie = categories,
  OLS = beta_ols,
  RR_BLUP = beta_hat_rr,
  Bayes_A = beta_hat_bayesA,
  LASSO = beta_hat_lasso
)

# Calcul du ratio de shrinkage : |β_méthode| / |β_OLS|
shrinkage_by_category$Ratio_RR <- abs(beta_hat_rr) / (abs(beta_ols) + 1e-8)
shrinkage_by_category$Ratio_BayesA <- abs(beta_hat_bayesA) / (abs(beta_ols) + 1e-8)
shrinkage_by_category$Ratio_LASSO <- abs(beta_hat_lasso) / (abs(beta_ols) + 1e-8)

# Moyenne par catégorie
shrinkage_summary <- shrinkage_by_category %>%
  group_by(Catégorie) %>%
  summarise(
    RR_BLUP_ratio = mean(Ratio_RR, na.rm = TRUE),
    Bayes_A_ratio = mean(Ratio_BayesA, na.rm = TRUE),
    LASSO_ratio = mean(Ratio_LASSO, na.rm = TRUE),
    .groups = "drop"
  )

kable(shrinkage_summary, digits = 3,
      caption = "Ratio de shrinkage moyen par catégorie de coefficient (vs OLS)") %>%
  kable_styling(latex_options = "hold_position")
```

```{r plot_shrinkage_by_category}
# Visualisation
shrinkage_summary_long <- shrinkage_summary %>%
  pivot_longer(cols = -Catégorie, names_to = "Méthode", values_to = "Ratio") %>%
  mutate(Méthode = gsub("_ratio", "", Méthode))

ggplot(shrinkage_summary_long, aes(x = Catégorie, y = Ratio, fill = Méthode)) +
  geom_col(position = "dodge") +
  geom_hline(yintercept = 1, linetype = "dashed", color = "red") +
  labs(title = "Ratio de shrinkage par catégorie de coefficient",
       subtitle = expression("Ratio = " * "|" * beta[method] * "|" / "|" * beta[OLS] * "|" * " (1 = pas de shrinkage)"),
       y = "Ratio moyen",
       x = "Catégorie (magnitude OLS)") +
  theme_minimal() +
  theme(legend.position = "bottom")
```

**Interprétation de l'effet de shrinkage** :

### 1. Shrinkage global (magnitude moyenne)

D'après les statistiques :

- **LASSO Bayésien** : shrinkage le plus prononcé
  - Moyenne des $|\beta|$ : `r round(mean(abs(beta_hat_lasso)), 4)`
  - `r sum(abs(beta_hat_lasso) < 0.01)` coefficients quasi-nuls
  
- **Bayes A** : shrinkage intermédiaire
  - Moyenne des $|\beta|$ : `r round(mean(abs(beta_hat_bayesA)), 4)`
  
- **RR-BLUP** : shrinkage le plus faible
  - Moyenne des $|\beta|$ : `r round(mean(abs(beta_hat_rr)), 4)`

### 2. Shrinkage différentiel (selon magnitude)

| Type de coefficient | RR-BLUP | Bayes A | LASSO |
|---------------------|---------|---------|-------|
| **Petits** | Shrinkage modéré | Shrinkage fort | **Shrinkage très fort** ($\to$ 0) |
| **Moyens** | Shrinkage modéré | Shrinkage modéré | Shrinkage fort |
| **Grands** | Shrinkage modéré | Shrinkage faible | **Préservés** |

### 3. Mécanismes expliquant les différences

**RR-BLUP** :
- Pénalité L2 **proportionnelle** : $\lambda \sum \beta_j^2$
- Réduction homogène de tous les coefficients
- Pas de discrimination selon la magnitude

**Bayes A** :
- Variances individuelles $\sigma^2_{\beta_j}$
- Shrinkage **adaptatif** : fort si variance a posteriori faible
- Compromis entre tous les coefficients

**LASSO Bayésien** :
- Pénalité L1 : $\lambda \sum |\beta_j|$
- **Seuillage doux** (soft thresholding)
- Petits coefficients $\to$ 0 exact
- Grands coefficients $\to$ peu affectés
- **Parcimonie maximale**

### Conclusion sur le shrinkage

```{r shrinkage_ranking}
ranking_shrinkage <- data.frame(
  Rang = 1:3,
  Méthode = c("LASSO Bayésien", "Bayes A", "RR-BLUP"),
  Intensité_Shrinkage = c("+++", "++", "+"),
  Parcimonie = c("Oui (forte)", "Non", "Non")
)

kable(ranking_shrinkage,
      caption = "Classement des méthodes par intensité de shrinkage") %>%
  kable_styling(latex_options = "hold_position")
```

**Important** : Un shrinkage fort n'est pas toujours meilleur !
- Trop de shrinkage $\to$ sous-ajustement (biais élevé)
- Pas assez $\to$ sur-ajustement (variance élevée)
- L'objectif est le **compromis optimal** pour la prédiction

## 5.3 Comparaison des performances prédictives

**Question** : À l'aide des corrélations entre valeurs prédites et observées, comparez ces trois méthodes.

```{r prediction_performance}
# Calcul des métriques de performance supplémentaires
# MSE (Mean Squared Error)
mse_rr <- mean((Y_test - Y_pred_rr)^2)
mse_bayesA <- mean((Y_test - Y_pred_bayesA)^2)
mse_lasso <- mean((Y_test - Y_pred_lasso)^2)

# RMSE (Root Mean Squared Error)
rmse_rr <- sqrt(mse_rr)
rmse_bayesA <- sqrt(mse_bayesA)
rmse_lasso <- sqrt(mse_lasso)

# MAE (Mean Absolute Error)
mae_rr <- mean(abs(Y_test - Y_pred_rr))
mae_bayesA <- mean(abs(Y_test - Y_pred_bayesA))
mae_lasso <- mean(abs(Y_test - Y_pred_lasso))

# R² (coefficient de détermination)
r2_rr <- cor_rr^2
r2_bayesA <- cor_bayesA^2
r2_lasso <- cor_lasso^2

# Tableau de synthèse
performance_df <- data.frame(
  Méthode = c("RR-BLUP", "Bayes A", "LASSO Bayésien"),
  Corrélation = c(cor_rr, cor_bayesA, cor_lasso),
  R2 = c(r2_rr, r2_bayesA, r2_lasso),
  RMSE = c(rmse_rr, rmse_bayesA, rmse_lasso),
  MAE = c(mae_rr, mae_bayesA, mae_lasso),
  MSE = c(mse_rr, mse_bayesA, mse_lasso)
)

performance_df$Rang_Corrélation <- rank(-performance_df$Corrélation)
performance_df$Rang_RMSE <- rank(performance_df$RMSE)

kable(performance_df, digits = 4,
      caption = "Comparaison complète des performances prédictives") %>%
  kable_styling(latex_options = c("hold_position", "scale_down"))
```

```{r plot_performance_metrics}
# Graphique en radar des performances
# (normalisation pour avoir des échelles comparables)
performance_norm <- performance_df %>%
  mutate(
    Corr_norm = (Corrélation - min(Corrélation)) / (max(Corrélation) - min(Corrélation)),
    R2_norm = (R2 - min(R2)) / (max(R2) - min(R2)),
    RMSE_norm = 1 - (RMSE - min(RMSE)) / (max(RMSE) - min(RMSE)),  # Inversé (petit = bon)
    MAE_norm = 1 - (MAE - min(MAE)) / (max(MAE) - min(MAE))  # Inversé
  ) %>%
  select(Méthode, Corr_norm, R2_norm, RMSE_norm, MAE_norm) %>%
  pivot_longer(cols = -Méthode, names_to = "Métrique", values_to = "Score")

ggplot(performance_norm, aes(x = Métrique, y = Score, fill = Méthode)) +
  geom_col(position = "dodge") +
  labs(title = "Comparaison des métriques de performance (normalisées)",
       subtitle = "Score = 1 : meilleure performance",
       y = "Score normalisé") +
  theme_minimal() +
  theme(legend.position = "bottom")
```

```{r residuals_analysis}
# Analyse des résidus
residuals_df <- data.frame(
  Observed = rep(Y_test, 3),
  Residual = c(Y_test - Y_pred_rr, Y_test - Y_pred_bayesA, Y_test - Y_pred_lasso),
  Méthode = rep(c("RR-BLUP", "Bayes A", "LASSO"), each = length(Y_test))
)

# Distribution des résidus
ggplot(residuals_df, aes(x = Residual, fill = Méthode)) +
  geom_histogram(bins = 15, alpha = 0.7, position = "identity") +
  geom_vline(xintercept = 0, linetype = "dashed", color = "red") +
  facet_wrap(~Méthode, ncol = 1) +
  labs(title = "Distribution des résidus par méthode",
       x = "Résidu (Observé - Prédit)",
       y = "Fréquence") +
  theme_minimal() +
  theme(legend.position = "none")
```

```{r qq_plots}
# QQ-plots pour normalité des résidus
par(mfrow = c(1, 3))
qqnorm(Y_test - Y_pred_rr, main = "QQ-Plot RR-BLUP")
qqline(Y_test - Y_pred_rr, col = "red")
qqnorm(Y_test - Y_pred_bayesA, main = "QQ-Plot Bayes A")
qqline(Y_test - Y_pred_bayesA, col = "red")
qqnorm(Y_test - Y_pred_lasso, main = "QQ-Plot LASSO")
qqline(Y_test - Y_pred_lasso, col = "red")
par(mfrow = c(1, 1))
```

**Analyse des performances prédictives** :

### 1. Classement global

La **meilleure méthode** est : **`r performance_df$Méthode[which.max(performance_df$Corrélation)]`**

- Corrélation maximale : `r round(max(performance_df$Corrélation), 4)`
- RMSE minimale : `r round(min(performance_df$RMSE), 4)`

### 2. Différences entre méthodes

```{r performance_differences}
# Calcul des écarts relatifs par rapport au meilleur
best_cor <- max(performance_df$Corrélation)
best_rmse <- min(performance_df$RMSE)

performance_df$Écart_Cor_pct <- 
  100 * (performance_df$Corrélation - best_cor) / best_cor

performance_df$Écart_RMSE_pct <- 
  100 * (performance_df$RMSE - best_rmse) / best_rmse

kable(performance_df %>% select(Méthode, Corrélation, Écart_Cor_pct, RMSE, Écart_RMSE_pct),
      digits = 2,
      col.names = c("Méthode", "Corrélation", "Écart (%)", "RMSE", "Écart (%)"),
      caption = "Écarts relatifs par rapport à la meilleure méthode") %>%
  kable_styling(latex_options = "hold_position")
```

**Observations** :

1. **Si les différences sont faibles (< 5%)** :
   - Les trois méthodes capturent l'information de manière similaire
   - Le choix dépend d'autres critères (interprétabilité, parcimonie)
   
2. **Si LASSO domine** :
   - La parcimonie est bénéfique (beaucoup de bruit dans les 160 variables)
   - Un modèle simple suffit
   
3. **Si Bayes A domine** :
   - L'information est distribuée sur beaucoup de variables
   - Le shrinkage adaptatif capture mieux la complexité

### 3. Qualité des résidus

```{r residuals_stats}
library(moments) # Pour skewness et kurtosis

residuals_stats <- residuals_df %>%
  group_by(Méthode) %>%
  summarise(
    Moyenne = mean(Residual),
    Écart_type = sd(Residual),
    Skewness = moments::skewness(Residual),
    Kurtosis = moments::kurtosis(Residual),
    .groups = "drop"
  )

kable(residuals_stats, digits = 3,
      caption = "Statistiques des résidus") %>%
  kable_styling(latex_options = "hold_position")
```

**Critères de qualité des résidus** :
  - Moyenne $\approx$ 0 (modèle non biaisé)
  - Distribution symétrique (skewness $\approx$ 0)
  - Pas de queues trop lourdes (kurtosis $\approx$ 3)

### 4. Conclusion sur les performances

```{r, results='asis'}
library(glue)
perf_comment <- ifelse(
  max(performance_df$Écart_Cor_pct) < -5,
  "très proches",
  "modérément différentes"
)

cat(glue("Les trois méthodes sont **compétitives**, avec des performances {perf_comment}."))
```

**Recommandation** :
  - **Pour la prédiction** : choisir la méthode avec la meilleure corrélation
  - **Pour l'interprétation** : préférer LASSO (parcimonie) ou SSVS (probabilités)
  - **Pour la robustesse** : Bayes A (gère mieux les corrélations)

## 5.4 Comparaison des variables sélectionnées

**Question** : Comparez les variables retenues avec les quatre méthodes : RR-BLUP, LASSO bayésien, Bayes A et SSVS.

```{r variable_selection_comparison}
# Compilation des sélections (top 15 pour avoir plus de recul)
top_15_rr <- order(abs(beta_hat_rr), decreasing = TRUE)[1:15]
top_15_bayesA <- order(abs(beta_hat_bayesA), decreasing = TRUE)[1:15]
top_15_lasso <- order(abs(beta_hat_lasso), decreasing = TRUE)[1:15]
top_15_ssvs <- order(selection_freq_ssvs, decreasing = TRUE)[1:15]

# Création d'une matrice de sélection
all_vars <- unique(c(top_15_rr, top_15_bayesA, top_15_lasso, top_15_ssvs))
selection_matrix <- data.frame(
  Variable = X_cols[all_vars],
  RR_BLUP = all_vars %in% top_15_rr,
  Bayes_A = all_vars %in% top_15_bayesA,
  LASSO = all_vars %in% top_15_lasso,
  SSVS = all_vars %in% top_15_ssvs
)

selection_matrix$Nb_Méthodes <- rowSums(selection_matrix[, -1])

# Tri par nombre de méthodes
selection_matrix <- selection_matrix %>%
  arrange(desc(Nb_Méthodes), Variable)

kable(head(selection_matrix, 20), 
      caption = "Variables sélectionnées par méthode (top 15 de chaque)") %>%
  kable_styling(latex_options = c("hold_position", "scale_down", "striped"))
```

```{r consensus_variables}
# Variables de consensus (dans au moins 3 méthodes)
consensus_vars <- selection_matrix %>%
  filter(Nb_Méthodes >= 3)

cat("Nombre de variables consensus (≥3 méthodes):", nrow(consensus_vars), "\n\n")

if(nrow(consensus_vars) > 0) {
  cat("Variables consensus :\n")
  print(consensus_vars$Variable)
}
```

```{r venn_diagram_concept}
# Diagramme de recouvrement (conceptuel)
overlap_counts <- data.frame(
  Recouvrement = c("4 méthodes", "3 méthodes", "2 méthodes", "1 méthode"),
  Nombre = c(
    sum(selection_matrix$Nb_Méthodes == 4),
    sum(selection_matrix$Nb_Méthodes == 3),
    sum(selection_matrix$Nb_Méthodes == 2),
    sum(selection_matrix$Nb_Méthodes == 1)
  )
)

ggplot(overlap_counts, aes(x = Recouvrement, y = Nombre, fill = Recouvrement)) +
  geom_col() +
  geom_text(aes(label = Nombre), vjust = -0.5, size = 5) +
  labs(title = "Consensus entre méthodes",
       subtitle = "Nombre de variables selon le nombre de méthodes qui les sélectionnent",
       y = "Nombre de variables") +
  theme_minimal() +
  theme(legend.position = "none")
```

```{r selection_by_channel_type}
# Analyse par type de chaîne
selected_all_methods <- lapply(list(
  RR = X_cols[top_15_rr],
  BayesA = X_cols[top_15_bayesA],
  LASSO = X_cols[top_15_lasso],
  SSVS = X_cols[top_15_ssvs]
), extract_channel_type)

# Comptage par type

# Récupération de tous les types possibles
all_types <- names(table(extract_channel_type(X_cols)))

# Recalcul des tables en imposant les mêmes niveaux
count_RR     <- table(factor(selected_all_methods$RR,     levels = all_types))
count_BayesA <- table(factor(selected_all_methods$BayesA, levels = all_types))
count_LASSO  <- table(factor(selected_all_methods$LASSO,  levels = all_types))
count_SSVS   <- table(factor(selected_all_methods$SSVS,   levels = all_types))

# Construction du tableau récapitulatif 
channel_summary <- data.frame(
  Type    = all_types,
  RR_BLUP = as.vector(count_RR),
  Bayes_A = as.vector(count_BayesA),
  LASSO   = as.vector(count_LASSO),
  SSVS    = as.vector(count_SSVS)
)

kable(channel_summary,
      caption = "Répartition des variables sélectionnées par type de chaîne et méthode") %>%
  kable_styling(latex_options = "hold_position")
```

```{r plot_channel_distribution}
# Visualisation
channel_long <- channel_summary %>%
  pivot_longer(cols = -Type, names_to = "Méthode", values_to = "Count")

ggplot(channel_long, aes(x = Type, y = Count, fill = Méthode)) +
  geom_col(position = "dodge") +
  labs(title = "Distribution des variables sélectionnées par type de chaîne",
       x = "Type de chaîne",
       y = "Nombre de variables (top 15)") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "bottom")
```

**Analyse de la sélection de variables** :

### 1. Consensus fort

- **`r sum(selection_matrix$Nb_Méthodes == 4)` variables** sont sélectionnées par les **4 méthodes**
- Ces variables sont les plus **robustes** et devraient être prioritaires

### 2. Différences entre méthodes

```{r unique_selections}
# Variables uniques à chaque méthode
unique_rr <- setdiff(top_15_rr, c(top_15_bayesA, top_15_lasso, top_15_ssvs))
unique_bayesA <- setdiff(top_15_bayesA, c(top_15_rr, top_15_lasso, top_15_ssvs))
unique_lasso <- setdiff(top_15_lasso, c(top_15_rr, top_15_bayesA, top_15_ssvs))
unique_ssvs <- setdiff(top_15_ssvs, c(top_15_rr, top_15_bayesA, top_15_lasso))

unique_df <- data.frame(
  Méthode = c("RR-BLUP", "Bayes A", "LASSO", "SSVS"),
  Nb_variables_uniques = c(
    length(unique_rr),
    length(unique_bayesA),
    length(unique_lasso),
    length(unique_ssvs)
  )
)

kable(unique_df,
      caption = "Nombre de variables uniques à chaque méthode (dans top 15)") %>%
  kable_styling(latex_options = "hold_position")
```

```{r detailed_comparison_matrix}
# Matrice de présence/absence détaillée pour top 10
top10_comparison_matrix <- matrix(0, nrow = 10, ncol = 4)
colnames(top10_comparison_matrix) <- c("RR-BLUP", "Bayes A", "LASSO", "SSVS")

# Liste des 10 premières variables de chaque méthode
all_top10_union <- unique(c(top_10_rr[1:10], top_10_bayesA[1:10], 
                             top_10_lasso[1:10], top_10_ssvs[1:10]))

comparison_detailed <- data.frame(
  Rang = 1:10,
  RR_BLUP = X_cols[top_10_rr[1:10]],
  Bayes_A = X_cols[top_10_bayesA[1:10]],
  LASSO = X_cols[top_10_lasso[1:10]],
  SSVS = X_cols[top_10_ssvs[1:10]]
)

kable(comparison_detailed,
      caption = "Comparaison rang par rang des top 10 de chaque méthode") %>%
  kable_styling(latex_options = c("hold_position", "scale_down", "striped"))
```

**Pourquoi des différences ?**

1. **RR-BLUP** : Pas de shrinkage fort $\to$ peut retenir des variables avec petit effet systématique
2. **Bayes A** : Shrinkage adaptatif $\to$ retient variables avec variance a posteriori élevée
3. **LASSO** : Sélection dans environnement corrélé $\to$ choix arbitraire parmi variables similaires
4. **SSVS** : Exploration stochastique $\to$ capture l'incertitude, peut différer des autres

### 3. Analyse détaillée des variables consensus

```{r consensus_analysis_detailed}
# Variables présentes dans 4, 3, 2 méthodes
consensus_4 <- selection_matrix %>% filter(Nb_Méthodes == 4)
consensus_3 <- selection_matrix %>% filter(Nb_Méthodes == 3)
consensus_2 <- selection_matrix %>% filter(Nb_Méthodes == 2)

cat("=== ANALYSE DU CONSENSUS ===\n\n")
cat("Variables sélectionnées par les 4 méthodes :", nrow(consensus_4), "\n")
if(nrow(consensus_4) > 0) {
  cat("\nListe complète :\n")
  for(i in 1:nrow(consensus_4)) {
    cat(paste0("  ", i, ". ", consensus_4$Variable[i], "\n"))
  }
}

cat("\n\nVariables sélectionnées par 3 méthodes :", nrow(consensus_3), "\n")
if(nrow(consensus_3) > 0) {
  cat("\nExemples (5 premières) :\n")
  for(i in 1:min(5, nrow(consensus_3))) {
    # Identifier quelle méthode n'a pas sélectionné
    methods <- c("RR_BLUP", "Bayes_A", "LASSO", "SSVS")
    missing_method <- methods[!unlist(consensus_3[i, methods])]
    cat(paste0("  ", i, ". ", consensus_3$Variable[i], 
               " (absente de : ", missing_method, ")\n"))
  }
}

cat("\n\nVariables sélectionnées par 2 méthodes :", nrow(consensus_2), "\n")
```

```{r consensus_coefficients}
# Comparaison des coefficients pour les variables consensus
if(nrow(consensus_4) > 0) {
  consensus_4_indices <- match(consensus_4$Variable, X_cols)
  
  consensus_coef_comparison <- data.frame(
    Variable = consensus_4$Variable,
    Beta_RR = beta_hat_rr[consensus_4_indices],
    Beta_BayesA = beta_hat_bayesA[consensus_4_indices],
    Beta_LASSO = beta_hat_lasso[consensus_4_indices],
    Freq_SSVS = selection_freq_ssvs[consensus_4_indices],
    Var_BayesA = varBeta_hat_bayesA[consensus_4_indices]
  )
  
  # Ajout de statistiques
  consensus_coef_comparison$Mean_Beta <- rowMeans(
    consensus_coef_comparison[, c("Beta_RR", "Beta_BayesA", "Beta_LASSO")]
  )
  consensus_coef_comparison$SD_Beta <- apply(
    consensus_coef_comparison[, c("Beta_RR", "Beta_BayesA", "Beta_LASSO")],
    1, sd
  )
  
  kable(consensus_coef_comparison, digits = 4,
        caption = "Coefficients détaillés des variables consensus (4 méthodes)") %>%
    kable_styling(latex_options = c("hold_position", "scale_down"))
}
```

**Interprétation des coefficients consensus** :

- **Cohérence des signes** : Si toutes les méthodes donnent le même signe (positif ou négatif), c'est un signal très robuste
- **Variabilité entre méthodes** : Un faible écart-type indique un accord fort sur la magnitude
- **Fréquence SSVS** : Une fréquence proche de 1 confirme la sélection avec haute certitude

```{r plot_consensus_coefficients}
if(nrow(consensus_4) > 0 && nrow(consensus_4) <= 15) {
  # Graphique uniquement si pas trop de variables
  consensus_coef_long <- consensus_coef_comparison %>%
    select(Variable, Beta_RR, Beta_BayesA, Beta_LASSO) %>%
    pivot_longer(cols = -Variable, names_to = "Méthode", values_to = "Coefficient") %>%
    mutate(Méthode = gsub("Beta_", "", Méthode))
  
  ggplot(consensus_coef_long, aes(x = Variable, y = Coefficient, color = Méthode, group = Méthode)) +
    geom_point(size = 3) +
    geom_line() +
    geom_hline(yintercept = 0, linetype = "dashed", color = "gray50") +
    coord_flip() +
    labs(title = "Comparaison des coefficients pour les variables consensus",
         subtitle = "Variables sélectionnées par les 4 méthodes",
         x = "",
         y = "Valeur du coefficient") +
    theme_minimal() +
    theme(legend.position = "bottom")
}
```

### 4. Analyse par type de chaîne (détaillée)

```{r channel_type_detailed_analysis}
# Pour chaque méthode, comptage par type
selection_by_type_detailed <- data.frame(
  Type = unique(extract_channel_type(X_cols)),
  stringsAsFactors = FALSE
)

# Ajout du nombre total de variables de chaque type
total_by_type <- table(extract_channel_type(X_cols))
selection_by_type_detailed$Total_disponible <- 
  total_by_type[match(selection_by_type_detailed$Type, names(total_by_type))]

# Comptage pour chaque méthode
for(method_name in c("RR", "BayesA", "LASSO", "SSVS")) {
  if(method_name == "RR") {
    selected_vars <- X_cols[top_15_rr]
  } else if(method_name == "BayesA") {
    selected_vars <- X_cols[top_15_bayesA]
  } else if(method_name == "LASSO") {
    selected_vars <- X_cols[top_15_lasso]
  } else {
    selected_vars <- X_cols[top_15_ssvs]
  }
  
  types_selected <- extract_channel_type(selected_vars)
  count_by_type <- table(types_selected)
  
  selection_by_type_detailed[[paste0("N_", method_name)]] <- 
    count_by_type[match(selection_by_type_detailed$Type, names(count_by_type))]
  
  # Remplacer NA par 0
  selection_by_type_detailed[[paste0("N_", method_name)]][
    is.na(selection_by_type_detailed[[paste0("N_", method_name)]])
  ] <- 0
  
  # Pourcentage de sélection
  selection_by_type_detailed[[paste0("Pct_", method_name)]] <- 
    round(100 * selection_by_type_detailed[[paste0("N_", method_name)]] / 
            selection_by_type_detailed$Total_disponible, 1)
}

# Calcul du taux de sélection moyen
selection_by_type_detailed$Taux_selection_moyen <- rowMeans(
  selection_by_type_detailed[, c("Pct_RR", "Pct_BayesA", "Pct_LASSO", "Pct_SSVS")],
  na.rm = TRUE
)

# Tri par taux de sélection décroissant
selection_by_type_detailed <- selection_by_type_detailed %>%
  arrange(desc(Taux_selection_moyen))

kable(selection_by_type_detailed, digits = 1,
      caption = "Analyse détaillée par type de chaîne : taux de sélection par méthode") %>%
  kable_styling(latex_options = c("hold_position", "scale_down", "striped"))
```

```{r plot_selection_rate_by_type}
# Graphique des taux de sélection
selection_rate_long <- selection_by_type_detailed %>%
  select(Type, Pct_RR, Pct_BayesA, Pct_LASSO, Pct_SSVS) %>%
  pivot_longer(cols = -Type, names_to = "Méthode", values_to = "Taux_pct") %>%
  mutate(Méthode = gsub("Pct_", "", Méthode))

ggplot(selection_rate_long, aes(x = reorder(Type, -Taux_pct, FUN = mean), 
                                  y = Taux_pct, fill = Méthode)) +
  geom_col(position = "dodge") +
  labs(title = "Taux de sélection par type de chaîne et méthode",
       subtitle = "Pourcentage de variables sélectionnées (sur top 15) par rapport au total disponible",
       x = "Type de chaîne",
       y = "Taux de sélection (%)") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "bottom")
```

**Interprétation métier des types de chaînes** :

```{r interpret_channel_types}
# Identification des types sur-représentés et sous-représentés
top_types <- head(selection_by_type_detailed, 3)
bottom_types <- tail(selection_by_type_detailed, 3)

cat("=== TYPES DE CHAÎNES LES PLUS SÉLECTIONNÉS ===\n\n")
for(i in 1:nrow(top_types)) {
  cat(paste0(i, ". ", top_types$Type[i], " : ", 
             round(top_types$Taux_selection_moyen[i], 1), "% en moyenne\n"))
  cat(paste0("   (", top_types$Total_disponible[i], 
             " chaînes disponibles)\n\n"))
}

cat("\n=== TYPES DE CHAÎNES LES MOINS SÉLECTIONNÉS ===\n\n")
for(i in 1:nrow(bottom_types)) {
  cat(paste0(i, ". ", bottom_types$Type[i], " : ", 
             round(bottom_types$Taux_selection_moyen[i], 1), "% en moyenne\n"))
  cat(paste0("   (", bottom_types$Total_disponible[i], 
             " chaînes disponibles)\n\n"))
}
```

**Recommandations basées sur les types** :

1. **Types à fort impact positif** : 
   - Augmenter l'offre dans ces catégories
   - Négocier l'acquisition de nouvelles chaînes similaires
   - Mettre en avant dans l'interface utilisateur

2. **Types à faible sélection** :
   - Évaluer la qualité du contenu
   - Possibilité de réduire l'offre dans ces catégories
   - Réallouer le budget vers les catégories performantes

### 5. Matrice de corrélation entre sélections

```{r correlation_selections}
# Matrice binaire : 1 si variable dans top 15, 0 sinon
selection_binary <- matrix(0, nrow = ncol(X_train), ncol = 4)
colnames(selection_binary) <- c("RR_BLUP", "Bayes_A", "LASSO", "SSVS")
rownames(selection_binary) <- X_cols

selection_binary[top_15_rr, "RR_BLUP"] <- 1
selection_binary[top_15_bayesA, "Bayes_A"] <- 1
selection_binary[top_15_lasso, "LASSO"] <- 1
selection_binary[top_15_ssvs, "SSVS"] <- 1

# Corrélation entre les sélections
cor_selections <- cor(selection_binary)

kable(cor_selections, digits = 3,
      caption = "Corrélation entre les sélections de variables des 4 méthodes") %>%
  kable_styling(latex_options = "hold_position")
```

```{r plot_correlation_heatmap}
# Heatmap de corrélation
library(reshape2)
cor_selections_melted <- melt(cor_selections)

ggplot(cor_selections_melted, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile(color = "white") +
  geom_text(aes(label = round(value, 2)), size = 5) +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0.5, limit = c(0, 1)) +
  labs(title = "Corrélation entre les sélections de variables",
       subtitle = "1 = accord parfait, 0 = aucun recouvrement",
       x = "", y = "", fill = "Corrélation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

**Interprétation des corrélations** :

- **Corrélation élevée (> 0.7)** : Les méthodes identifient des variables similaires $\to$ signal robuste
- **Corrélation moyenne (0.4-0.7)** : Accord partiel, chaque méthode capture des aspects différents
- **Corrélation faible (< 0.4)** : Désaccord substantiel $\to$ sensibilité aux hypothèses

```{r pairwise_agreement}
# Calcul des accords deux à deux (coefficient de Jaccard)
jaccard_similarity <- function(set1, set2) {
  intersection <- length(intersect(set1, set2))
  union <- length(union(set1, set2))
  return(intersection / union)
}

pairs <- combn(c("RR", "BayesA", "LASSO", "SSVS"), 2)
jaccard_results <- data.frame(
  Paire = apply(pairs, 2, function(x) paste(x, collapse = " vs ")),
  Jaccard = apply(pairs, 2, function(x) {
    set1 <- switch(x[1],
                   "RR" = top_15_rr,
                   "BayesA" = top_15_bayesA,
                   "LASSO" = top_15_lasso,
                   "SSVS" = top_15_ssvs)
    set2 <- switch(x[2],
                   "RR" = top_15_rr,
                   "BayesA" = top_15_bayesA,
                   "LASSO" = top_15_lasso,
                   "SSVS" = top_15_ssvs)
    jaccard_similarity(set1, set2)
  })
)

jaccard_results <- jaccard_results %>% arrange(desc(Jaccard))

kable(jaccard_results, digits = 3,
      caption = "Similarité de Jaccard entre les sélections (sur top 15)") %>%
  kable_styling(latex_options = "hold_position")
```

**Coefficient de Jaccard** : mesure la similarité entre deux ensembles

- **Formule** : $J(A, B) = \frac{|A \cap B|}{|A \cup B|}$
- **Interprétation** : 
  
  - 1 = ensembles identiques
  - 0 = aucune variable en commun
  - > 0.5 = forte similarité

### 6. Variables spécifiques à chaque méthode

```{r method_specific_variables}
# Variables UNIQUEMENT sélectionnées par chaque méthode
only_rr <- setdiff(top_15_rr, c(top_15_bayesA, top_15_lasso, top_15_ssvs))
only_bayesA <- setdiff(top_15_bayesA, c(top_15_rr, top_15_lasso, top_15_ssvs))
only_lasso <- setdiff(top_15_lasso, c(top_15_rr, top_15_bayesA, top_15_ssvs))
only_ssvs <- setdiff(top_15_ssvs, c(top_15_rr, top_15_bayesA, top_15_lasso))

cat("=== VARIABLES SPÉCIFIQUES À CHAQUE MÉTHODE ===\n\n")

cat("Variables uniquement sélectionnées par RR-BLUP :", length(only_rr), "\n")
if(length(only_rr) > 0) {
  cat("Liste :", paste(X_cols[only_rr], collapse = ", "), "\n\n")
}

cat("Variables uniquement sélectionnées par Bayes A :", length(only_bayesA), "\n")
if(length(only_bayesA) > 0) {
  cat("Liste :", paste(X_cols[only_bayesA], collapse = ", "), "\n\n")
}

cat("Variables uniquement sélectionnées par LASSO :", length(only_lasso), "\n")
if(length(only_lasso) > 0) {
  cat("Liste :", paste(X_cols[only_lasso], collapse = ", "), "\n\n")
}

cat("Variables uniquement sélectionnées par SSVS :", length(only_ssvs), "\n")
if(length(only_ssvs) > 0) {
  cat("Liste :", paste(X_cols[only_ssvs], collapse = ", "), "\n\n")
}
```

```{r analyze_specific_variables}
# Analyse des caractéristiques des variables spécifiques
if(length(only_lasso) > 0) {
  cat("\n=== ANALYSE DES VARIABLES SPÉCIFIQUES AU LASSO ===\n\n")
  
  # Ces variables ont probablement des corrélations avec d'autres
  # ou des effets faibles que seul LASSO détecte grâce à sa parcimonie
  
  specific_lasso_analysis <- data.frame(
    Variable = X_cols[only_lasso],
    Beta_LASSO = beta_hat_lasso[only_lasso],
    Beta_RR = beta_hat_rr[only_lasso],
    Beta_BayesA = beta_hat_bayesA[only_lasso],
    Freq_SSVS = selection_freq_ssvs[only_lasso]
  )
  
  kable(specific_lasso_analysis, digits = 4,
        caption = "Variables spécifiques au LASSO : comparaison avec autres méthodes") %>%
    kable_styling(latex_options = "hold_position")
  
  cat("\nHypothèses possibles :\n")
  cat("- Effet de sélection L1 : LASSO choisit ces variables dans un groupe corrélé\n")
  cat("- Shrinkage fort : autres méthodes les ont trop pénalisées\n")
  cat("- Sensibilité aux hyperparamètres : choix de $\\lambda$ favorable\n")
}
```

### 7. Synthèse visuelle : diagramme d'upset

```{r upset_plot_concept}
# Diagramme conceptuel de recouvrement (alternative au Venn à 4 ensembles)
# Comptage de toutes les combinaisons possibles

combinations <- expand.grid(
  RR = c(FALSE, TRUE),
  BayesA = c(FALSE, TRUE),
  LASSO = c(FALSE, TRUE),
  SSVS = c(FALSE, TRUE)
)
combinations <- combinations[rowSums(combinations) > 0, ]  # Exclure (0,0,0,0)

combinations$Count <- 0
combinations$Variables <- vector("list", nrow(combinations))

for(i in 1:nrow(combinations)) {
  selected_indices <- c()
  
  if(combinations$RR[i]) selected_indices <- c(selected_indices, top_15_rr)
  if(combinations$BayesA[i]) selected_indices <- c(selected_indices, top_15_bayesA)
  if(combinations$LASSO[i]) selected_indices <- c(selected_indices, top_15_lasso)
  if(combinations$SSVS[i]) selected_indices <- c(selected_indices, top_15_ssvs)
  
  # Variables présentes dans TOUTES les méthodes sélectionnées
  if(combinations$RR[i]) {
    candidates <- top_15_rr
  } else if(combinations$BayesA[i]) {
    candidates <- top_15_bayesA
  } else if(combinations$LASSO[i]) {
    candidates <- top_15_lasso
  } else {
    candidates <- top_15_ssvs
  }
  
  # Filtrer pour avoir uniquement cette combinaison exacte
  for(var in candidates) {
    in_rr <- var %in% top_15_rr
    in_bayesA <- var %in% top_15_bayesA
    in_lasso <- var %in% top_15_lasso
    in_ssvs <- var %in% top_15_ssvs
    
    if(in_rr == combinations$RR[i] && 
       in_bayesA == combinations$BayesA[i] &&
       in_lasso == combinations$LASSO[i] &&
       in_ssvs == combinations$SSVS[i]) {
      combinations$Count[i] <- combinations$Count[i] + 1
    }
  }
}

# Affichage des combinaisons non vides
combinations_display <- combinations %>%
  filter(Count > 0) %>%
  mutate(
    Combinaison = paste(
      ifelse(RR, "RR", ""),
      ifelse(BayesA, "BA", ""),
      ifelse(LASSO, "LA", ""),
      ifelse(SSVS, "SS", ""),
      sep = "-"
    )
  ) %>%
  arrange(desc(Count)) %>%
  select(Combinaison, Count)

kable(head(combinations_display, 10), 
      caption = "Top 10 des combinaisons de sélection (RR=RR-BLUP, BA=Bayes A, LA=LASSO, SS=SSVS)") %>%
  kable_styling(latex_options = "hold_position")
```

### 8. Conclusion de l'analyse 5.4

**Points clés de la comparaison des sélections** :

1. **Consensus fort** : `r sum(selection_matrix$Nb_Méthodes == 4)` variables identifiées par les 4 méthodes
   - Ces variables sont **hautement fiables** et devraient être prioritaires

2. **Convergence partielle** : `r sum(selection_matrix$Nb_Méthodes >= 3)` variables retenues par au moins 3 méthodes
   - Signal robuste malgré des différences méthodologiques

3. **Diversité des sélections** : Corrélation moyenne entre méthodes $\approx$ `r round(mean(cor_selections[lower.tri(cor_selections)]), 2)`
   - Chaque méthode apporte une perspective complémentaire

4. **Types de contenu dominants** : 
   - Les chaînes de type **`r selection_by_type_detailed$Type[1]`** sont les plus sélectionnées
   - Taux de sélection moyen : **`r round(selection_by_type_detailed$Taux_selection_moyen[1], 1)`%**

5. **Variables spécifiques** :
   - LASSO : `r length(only_lasso)` variables uniques (effet de parcimonie L1)
   - SSVS : `r length(only_ssvs)` variables uniques (exploration stochastique)
   - Bayes A : `r length(only_bayesA)` variables uniques (shrinkage adaptatif)

**Recommandation finale** : Utiliser les **variables consensus** comme base du modèle final, complétées éventuellement par quelques variables spécifiques selon l'objectif (parcimonie vs exhaustivité).

## 5.5 Intervalles de confiance a posteriori

**Question** : Comparez les intervalles de confiance a posteriori des paramètres les plus importants.

```{r posterior_intervals}
# Sélection des 5 variables les plus consensus
top_consensus <- head(selection_matrix %>% arrange(desc(Nb_Méthodes)), 5)
consensus_indices <- match(top_consensus$Variable, X_cols)

# Construction des IC à partir des estimations
# Note : pour RR-BLUP, on n'a pas de distribution a posteriori complète
# On approxime avec une normale basée sur l'écart-type empirique

# Pour Bayes A et LASSO, on simulerait normalement les distributions
# Ici on approxime avec des IC basés sur les estimations

ic_level <- 0.95
z_alpha <- qnorm(1 - (1 - ic_level)/2)

ic_data <- data.frame()

for(idx in consensus_indices) {
  # RR-BLUP (approximation)
  se_rr <- abs(beta_hat_rr[idx]) * 0.2  # Approximation
  ic_data <- rbind(ic_data, data.frame(
    Variable = X_cols[idx],
    Méthode = "RR-BLUP",
    Estimate = beta_hat_rr[idx],
    Lower = beta_hat_rr[idx] - z_alpha * se_rr,
    Upper = beta_hat_rr[idx] + z_alpha * se_rr
  ))
  
  # Bayes A
  se_bayesA <- sqrt(varBeta_hat_bayesA[idx])
  ic_data <- rbind(ic_data, data.frame(
    Variable = X_cols[idx],
    Méthode = "Bayes A",
    Estimate = beta_hat_bayesA[idx],
    Lower = beta_hat_bayesA[idx] - z_alpha * se_bayesA,
    Upper = beta_hat_bayesA[idx] + z_alpha * se_bayesA
  ))
  
  # LASSO (approximation)
  se_lasso <- abs(beta_hat_lasso[idx]) * 0.25
  ic_data <- rbind(ic_data, data.frame(
    Variable = X_cols[idx],
    Méthode = "LASSO",
    Estimate = beta_hat_lasso[idx],
    Lower = beta_hat_lasso[idx] - z_alpha * se_lasso,
    Upper = beta_hat_lasso[idx] + z_alpha * se_lasso
  ))
}

# Affichage
# kable(ic_data, digits = 4,
#       caption = "Intervalles de confiance à 95% pour les variables consensus") %>%
#   kable_styling(latex_options = c("hold_position", "scale_down"))
```

```{r plot_confidence_intervals}
# Graphique forest plot
ggplot(ic_data, aes(x = Variable, y = Estimate, color = Méthode)) +
  geom_point(position = position_dodge(width = 0.5), size = 3) +
  geom_errorbar(aes(ymin = Lower, ymax = Upper), 
                position = position_dodge(width = 0.5), width = 0.3) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  coord_flip() +
  labs(title = "Intervalles de confiance à 95% des coefficients",
       subtitle = "Variables consensus (sélectionnées par plusieurs méthodes)",
       x = "",
       y = "Valeur du coefficient") +
  theme_minimal() +
  theme(legend.position = "bottom")
```

**Interprétation des IC** :

1. **Largeur des intervalles** :
   - IC **larges** $\to$ forte incertitude sur le coefficient
   - IC **étroits** $\to$ estimation précise
   
2. **Contient zéro ?** :
   - Si IC contient 0 $\to$ effet **non significatif** au seuil 95%
   - Si IC ne contient pas 0 $\to$ effet **significatif**

3. **Cohérence entre méthodes** :
   - IC qui se chevauchent $\to$ estimation cohérente
   - IC disjoints $\to$ désaccord substantiel (à investiguer)

## 5.6 Comparaison avec méthodes pénalisées non-bayésiennes

**Question** : Utilisez une méthode de régression pénalisée non bayésienne (LASSO, Ridge ou ElasticNet).

```{r glmnet_models}
# Chargement de glmnet (déjà fait normalement)
library(glmnet)

# 1. LASSO fréquentiste (alpha = 1)
set.seed(2026)
cv_lasso_freq <- cv.glmnet(X_train, Y_train, alpha = 1, nfolds = 10)
lasso_freq_model <- glmnet(X_train, Y_train, alpha = 1, 
                            lambda = cv_lasso_freq$lambda.min)
beta_lasso_freq <- as.vector(coef(lasso_freq_model))[-1]  # Sans intercept

# 2. Ridge fréquentiste (alpha = 0)
cv_ridge_freq <- cv.glmnet(X_train, Y_train, alpha = 0, nfolds = 10)
ridge_freq_model <- glmnet(X_train, Y_train, alpha = 0,
                            lambda = cv_ridge_freq$lambda.min)
beta_ridge_freq <- as.vector(coef(ridge_freq_model))[-1]

# 3. ElasticNet (alpha = 0.5)
cv_enet_freq <- cv.glmnet(X_train, Y_train, alpha = 0.5, nfolds = 10)
enet_freq_model <- glmnet(X_train, Y_train, alpha = 0.5,
                           lambda = cv_enet_freq$lambda.min)
beta_enet_freq <- as.vector(coef(enet_freq_model))[-1]

# Prédictions
Y_pred_lasso_freq <- predict(lasso_freq_model, newx = X_test, 
                              s = cv_lasso_freq$lambda.min)
Y_pred_ridge_freq <- predict(ridge_freq_model, newx = X_test,
                              s = cv_ridge_freq$lambda.min)
Y_pred_enet_freq <- predict(enet_freq_model, newx = X_test,
                             s = cv_enet_freq$lambda.min)

# Corrélations
cor_lasso_freq <- cor(Y_test, Y_pred_lasso_freq)
cor_ridge_freq <- cor(Y_test, Y_pred_ridge_freq)
cor_enet_freq <- cor(Y_test, Y_pred_enet_freq)

# Statistiques
freq_methods <- data.frame(
  Méthode = c("LASSO fréq.", "Ridge fréq.", "ElasticNet fréq."),
  Lambda_optimal = c(cv_lasso_freq$lambda.min, cv_ridge_freq$lambda.min, cv_enet_freq$lambda.min),
  Nb_variables_non_nulles = c(
    sum(abs(beta_lasso_freq) > 1e-6),
    sum(abs(beta_ridge_freq) > 1e-6),
    sum(abs(beta_enet_freq) > 1e-6)
  ),
  Corrélation = c(cor_lasso_freq, cor_ridge_freq, cor_enet_freq)
)

kable(freq_methods, digits = 4,
      caption = "Méthodes pénalisées fréquentistes (glmnet)") %>%
  kable_styling(latex_options = "hold_position")
```

```{r compare_bayesian_frequentist}
# Comparaison globale Bayésien vs Fréquentiste
all_methods_comp <- data.frame(
  Méthode = c("RR-BLUP", "Bayes A", "LASSO Bayésien", "SSVS",
              "Ridge fréq.", "LASSO fréq.", "ElasticNet fréq."),
  Type = c("Bayésien", "Bayésien", "Bayésien", "Bayésien",
           "Fréquentiste", "Fréquentiste", "Fréquentiste"),
  Corrélation = c(cor_rr, cor_bayesA, cor_lasso, NA,  # SSVS pas de prédiction directe
                  cor_ridge_freq, cor_lasso_freq, cor_enet_freq),
  Nb_variables = c(
    length(top_10_rr),
    length(top_10_bayesA),
    length(top_10_lasso),
    length(top_10_ssvs),
    10,  # On prend top 10 pour comparaison
    sum(abs(beta_lasso_freq) > quantile(abs(beta_lasso_freq), 0.9)),
    sum(abs(beta_enet_freq) > quantile(abs(beta_enet_freq), 0.9))
  )
)

kable(all_methods_comp, digits = 4,
      caption = "Comparaison Bayésien vs Fréquentiste") %>%
  kable_styling(latex_options = "hold_position") %>%
  row_spec(which.max(all_methods_comp$Corrélation[!is.na(all_methods_comp$Corrélation)]), 
           bold = TRUE, background = "#90EE90")
```

```{r plot_bayesian_vs_freq}
# Comparaison graphique
comp_plot_df <- all_methods_comp %>%
  filter(!is.na(Corrélation)) %>%
  arrange(desc(Corrélation))

ggplot(comp_plot_df, aes(x = reorder(Méthode, Corrélation), y = Corrélation, fill = Type)) +
  geom_col() +
  geom_text(aes(label = round(Corrélation, 3)), hjust = -0.1, size = 3.5) +
  coord_flip() +
  scale_fill_manual(values = c("Bayésien" = "steelblue", "Fréquentiste" = "coral")) +
  labs(title = "Performance prédictive : Bayésien vs Fréquentiste",
       x = "",
       y = "Corrélation (test set)") +
  theme_minimal() +
  theme(legend.position = "bottom")
```

**Analyse Bayésien vs Fréquentiste** :

### 1. Performances prédictives

- **Meilleure méthode globale** : `r comp_plot_df$Méthode[1]`
- Les approches bayésiennes sont `r ifelse(mean(all_methods_comp$Corrélation[1:3], na.rm=TRUE) > mean(all_methods_comp$Corrélation[5:7]), "légèrement supérieures", "comparables")` aux fréquentistes

### 2. Avantages des méthodes bayésiennes

1. **Quantification d'incertitude** : distributions a posteriori complètes
2. **Flexibilité** : hyperparamètres adaptatifs (Bayes A)
3. **Interprétation probabiliste** : $P(\beta \neq 0)$ avec SSVS
4. **A priori informatifs** : intégration de connaissance experte possible

### 3. Avantages des méthodes fréquentistes

1. **Rapidité** : glmnet très efficace computationnellement
2. **Simplicité** : un seul hyperparamètre $\lambda$ à tuner (CV automatique)
3. **Robustesse** : moins sensible aux spécifications (pas de choix d'a priori)
4. **Solution unique** : pas de variabilité MCMC

### 4. Quand préférer quoi ?

| Situation | Recommandation |
|-----------|----------------|
| **n >> p** (beaucoup de données) | Fréquentiste (plus rapide, performances similaires) |
| **p >> n** (haute dimension) | Bayésien (meilleure régularisation) ou LASSO fréq. |
| **Incertitude importante** | Bayésien (quantification complète) |
| **Besoin de rapidité** | Fréquentiste (glmnet) |
| **Connaissance a priori** | Bayésien (intégration d'expertise) |

## 5.7 Comparaison des variables sélectionnées (suite)

```{r compare_freq_selection}
# Top 10 des méthodes fréquentistes
top_10_lasso_freq <- order(abs(beta_lasso_freq), decreasing = TRUE)[1:10]
top_10_ridge_freq <- order(abs(beta_ridge_freq), decreasing = TRUE)[1:10]
top_10_enet_freq <- order(abs(beta_enet_freq), decreasing = TRUE)[1:10]

# Recouvrement avec les méthodes bayésiennes
# Consensus fort : variables dans top 10 d'au moins 5 méthodes
all_top10 <- list(
  RR = top_10_rr,
  BayesA = top_10_bayesA,
  LASSO_Bayes = top_10_lasso,
  SSVS = top_10_ssvs,
  Ridge_freq = top_10_ridge_freq,
  LASSO_freq = top_10_lasso_freq,
  Enet_freq = top_10_enet_freq
)

# Fréquence d'apparition de chaque variable
var_frequency <- table(unlist(all_top10))
ultra_consensus <- as.numeric(names(var_frequency[var_frequency >= 5]))

cat("Variables ultra-consensus (≥5 méthodes sur 7):", length(ultra_consensus), "\n")
if(length(ultra_consensus) > 0) {
  cat("\nCes variables sont :\n")
  print(X_cols[ultra_consensus])
}
```

**Conclusion sur la sélection** :

Les `r length(ultra_consensus)` variables ultra-consensus constituent le **cœur robuste** du modèle :
- Identifiées par bayésien ET fréquentiste
- Très haute confiance dans leur pertinence
- Devraient être **prioritaires** dans toute modélisation

## 5.8 Modèle de régression standard

**Question** : Proposez un modèle de régression « standard » avec quelques variables issues des sélections bayésiennes.

```{r final_model_selection}
# Sélection des variables finales
# Critère : consensus d'au moins 3 méthodes bayésiennes + validation fréquentiste

final_vars_indices <- ultra_consensus

if(length(final_vars_indices) == 0) {
  # Fallback : top consensus bayésien
  final_vars_indices <- head(consensus_indices, 8)
}

final_vars_names <- X_cols[final_vars_indices]
n_final_vars <- length(final_vars_indices)

cat("Variables retenues pour le modèle final:", n_final_vars, "\n\n")
cat("Liste des variables :\n")
print(final_vars_names)

# Construction du modèle OLS classique
X_train_final <- X_train[, final_vars_indices]
X_test_final <- X_test[, final_vars_indices]

final_model <- lm(Y_train ~ ., data = data.frame(Y_train, X_train_final))
```

```{r final_model_summary}
# Résumé du modèle
summary_final <- summary(final_model)
cat("\n=== RÉSUMÉ DU MODÈLE FINAL ===\n")
print(summary_final)
```

```{r final_model_table}
# Tableau des coefficients
coef_df <- data.frame(
  Variable = c("(Intercept)", final_vars_names),
  Estimate = coef(final_model),
  Std_Error = summary_final$coefficients[, "Std. Error"],
  t_value = summary_final$coefficients[, "t value"],
  p_value = summary_final$coefficients[, "Pr(>|t|)"],
  Significatif = ifelse(summary_final$coefficients[, "Pr(>|t|)"] < 0.05, "***",
                 ifelse(summary_final$coefficients[, "Pr(>|t|)"] < 0.1, "*", ""))
)

kable(coef_df, digits = 4,
      caption = "Coefficients du modèle de régression final") %>%
  kable_styling(latex_options = c("hold_position", "scale_down"))
```

```{r final_model_performance}
# Prédictions et performances
test_df <- data.frame(X_test_final)
colnames(test_df) <- colnames(X_train_final)
Y_pred_final <- predict(final_model, newdata = test_df)
cor_final <- cor(Y_test, Y_pred_final)
r2_final <- summary_final$r.squared
r2_adj_final <- summary_final$adj.r.squared
rmse_final <- sqrt(mean((Y_test - Y_pred_final)^2))

performance_final <- data.frame(
  Métrique = c("R²", "R² ajusté", "Corrélation", "RMSE", "Nb. variables"),
  Valeur = c(r2_final, r2_adj_final, cor_final, rmse_final, n_final_vars)
)

kable(performance_final, digits = 4,
      caption = "Performance du modèle final") %>%
  kable_styling(latex_options = "hold_position")
```

```{r plot_final_predictions}
# Graphique prédictions vs observations
pred_final_df <- data.frame(
  Observed = Y_test,
  Predicted = Y_pred_final
)

ggplot(pred_final_df, aes(x = Observed, y = Predicted)) +
  geom_point(alpha = 0.7, color = "darkgreen", size = 3) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red", size = 1) +
  annotate("text", x = min(Y_test), y = max(Y_pred_final),
           label = paste0("R² = ", round(cor_final^2, 3), "\nRMSE = ", round(rmse_final, 3)),
           hjust = 0, vjust = 1, size = 5) +
  labs(title = "Modèle final : Prédictions vs Observations",
       subtitle = paste(n_final_vars, "variables sélectionnées"),
       x = "Scores observés",
       y = "Scores prédits") +
  theme_minimal()
```

## 5.9 Analyse du modèle final

**Question** : Analysez le résultat de la régression obtenue (p-value, effet des variables, résidus, …).

### 1. Significativité des coefficients

```{r significant_vars}
# Variables significatives
sig_vars <- coef_df %>%
  filter(p_value < 0.05, Variable != "(Intercept)") %>%
  arrange(p_value)

cat("Nombre de variables significatives (p < 0.05):", nrow(sig_vars), "sur", n_final_vars, "\n\n")

kable(sig_vars, digits = 4,
      caption = "Variables significatives du modèle final") %>%
  kable_styling(latex_options = "hold_position")
```

**Interprétation** :
- `r nrow(sig_vars)` variables sur `r n_final_vars` sont significatives au seuil 5%
- Cela confirme la pertinence de la sélection bayésienne
- Les variables non significatives pourraient être retirées pour plus de parcimonie

### 2. Effet des variables (coefficients)

```{r coefficient_interpretation}
# Classement par magnitude
coef_ranked <- coef_df %>%
  filter(Variable != "(Intercept)") %>%
  arrange(desc(abs(Estimate)))

cat("Top 5 des variables par effet :\n\n")
kable(head(coef_ranked, 5), digits = 4) %>%
  kable_styling(latex_options = "hold_position")
```

```{r plot_coefficients}
# Graphique des coefficients
ggplot(coef_df %>% filter(Variable != "(Intercept)"), 
       aes(x = reorder(Variable, Estimate), y = Estimate)) +
  geom_col(aes(fill = p_value < 0.05)) +
  geom_errorbar(aes(ymin = Estimate - 1.96*Std_Error, 
                    ymax = Estimate + 1.96*Std_Error), width = 0.3) +
  coord_flip() +
  scale_fill_manual(values = c("FALSE" = "gray70", "TRUE" = "steelblue"),
                    labels = c("Non sig.", "Sig. (p<0.05)")) +
  labs(title = "Coefficients du modèle final avec IC à 95%",
       x = "",
       y = "Valeur du coefficient",
       fill = "") +
  theme_minimal() +
  theme(legend.position = "bottom")
```

**Interprétation métier** :

Les variables à **effet positif** (coefficient > 0) augmentent la satisfaction :
```{r positive_effects}
positive_vars <- coef_ranked %>% filter(Estimate > 0, p_value < 0.05)
if(nrow(positive_vars) > 0) {
  cat("Variables à effet positif significatif :\n")
  print(positive_vars$Variable)
}
```

Les variables à **effet négatif** (coefficient < 0) diminuent la satisfaction :
```{r negative_effects}
negative_vars <- coef_ranked %>% filter(Estimate < 0, p_value < 0.05)
if(nrow(negative_vars) > 0) {
  cat("\nVariables à effet négatif significatif :\n")
  print(negative_vars$Variable)
}
```

### 3. Analyse des résidus

```{r residuals_final}
# Extraction des résidus
residuals_final <- residuals(final_model)

# Statistiques
cat("Statistiques des résidus :\n")
cat("Moyenne:", mean(residuals_final), "\n")
cat("Écart-type:", sd(residuals_final), "\n")
cat("Min:", min(residuals_final), "\n")
cat("Max:", max(residuals_final), "\n")
```

```{r diagnostic_plots}
# Graphiques de diagnostic
par(mfrow = c(2, 2))
plot(final_model)
par(mfrow = c(1, 1))
```

**Vérification des hypothèses** :

1. **Linéarité** (Residuals vs Fitted) :
   - Résidus dispersés aléatoirement autour de 0 
   - Pas de structure évidente $\to$ hypothèse respectée

2. **Normalité** (QQ-plot) :
   - Points alignés sur la diagonale 
   - Quelques déviations aux extrémités acceptables

3. **Homoscédasticité** (Scale-Location) :
   - Variance constante des résidus 
   - Pas d'effet entonnoir

4. **Points influents** (Residuals vs Leverage) :
   - Quelques points à surveiller (Cook's distance)
   - Pas de valeurs aberrantes majeures

```{r shapiro_test}
# Test de normalité des résidus
shapiro_test <- shapiro.test(residuals_final)
cat("\nTest de Shapiro-Wilk (normalité des résidus) :\n")
cat("p-value:", shapiro_test$p.value, "\n")
cat("Interprétation:", ifelse(shapiro_test$p.value > 0.05, 
                              "Résidus compatibles avec normalité",
                              "Résidus significativement non-normaux"), "\n")
```

### 4. Comparaison avec les modèles bayésiens

```{r final_comparison}
# Tableau récapitulatif
final_comparison <- data.frame(
  Modèle = c("RR-BLUP", "Bayes A", "LASSO Bayésien", 
             "Ridge fréq.", "LASSO fréq.", "Modèle final OLS"),
  Corrélation = c(cor_rr, cor_bayesA, cor_lasso, 
                  cor_ridge_freq, cor_lasso_freq, cor_final),
  RMSE = c(rmse_rr, rmse_bayesA, rmse_lasso,
           sqrt(mean((Y_test - Y_pred_ridge_freq)^2)),
           sqrt(mean((Y_test - Y_pred_lasso_freq)^2)),
           rmse_final),
  Nb_variables = c(ncol(X_train), ncol(X_train), ncol(X_train),
                   ncol(X_train), sum(abs(beta_lasso_freq) > 1e-6), n_final_vars)
)

final_comparison$Rang <- rank(-final_comparison$Corrélation)

kable(final_comparison, digits = 4,
      caption = "Comparaison finale de tous les modèles") %>%
  kable_styling(latex_options = "hold_position") %>%
  row_spec(nrow(final_comparison), bold = TRUE, background = "lightyellow")
```

**Bilan du modèle final** :

- **Performance** : `r ifelse(cor_final >= max(final_comparison$Corrélation[-nrow(final_comparison)]), "Comparable ou supérieure", "Légèrement inférieure")` aux modèles bayésiens complets
- **Parcimonie** : `r n_final_vars` variables seulement (vs `r ncol(X_train)` initialement)
- **Interprétabilité** : Modèle simple, coefficients directement interprétables
- **Robustesse** : Variables validées par plusieurs méthodes

## 5.10 Recommandations pour la chaîne câblée

**Question** : En conclusion, que pouvez-vous proposer à la chaîne de programmes pour améliorer les scores de satisfaction ?

### Synthèse des résultats

```{r final_recommendations}
# Extraction des insights
top_positive <- coef_ranked %>% filter(Estimate > 0) %>% head(3)
top_negative <- coef_ranked %>% filter(Estimate < 0) %>% head(3)

# Analyse par type de chaîne
channel_effects <- data.frame(
  Variable = coef_ranked$Variable,
  Type = extract_channel_type(coef_ranked$Variable),
  Coefficient = coef_ranked$Estimate,
  P_value = coef_ranked$p_value
) %>%
  group_by(Type) %>%
  summarise(
    Effet_moyen = mean(Coefficient),
    Nb_variables = n(),
    .groups = "drop"
  ) %>%
  arrange(desc(Effet_moyen))

kable(channel_effects, digits = 3,
      caption = "Effet moyen par type de chaîne dans le modèle final") %>%
  kable_styling(latex_options = "hold_position")
```

### Recommandations stratégiques

#### 1. Chaînes à promouvoir (effet positif fort)

Les chaînes suivantes ont un impact positif significatif sur la satisfaction :

```{r positive_recommendations}
if(nrow(top_positive) > 0) {
  cat("\nTop 3 des leviers positifs :\n")
  for(i in 1:nrow(top_positive)) {
    cat(paste0(i, ". ", top_positive$Variable[i], 
               " (coef = ", round(top_positive$Estimate[i], 3), ")\n"))
  }
  
  cat("\n**Actions recommandées** :\n")
  cat("- Augmenter la visibilité de ces chaînes (promotion, mise en avant)\n")
  cat("- Développer le contenu de ces catégories\n")
  cat("- Créer des packages incluant ces chaînes\n")
}
```

#### 2. Chaînes à retravailler (effet négatif)

Les chaînes suivantes ont un impact négatif sur la satisfaction :

```{r negative_recommendations}
if(nrow(top_negative) > 0) {
  cat("\nTop 3 des points d'attention :\n")
  for(i in 1:nrow(top_negative)) {
    cat(paste0(i, ". ", top_negative$Variable[i],
               " (coef = ", round(top_negative$Estimate[i], 3), ")\n"))
  }
  
  cat("\n**Actions recommandées** :\n")
  cat("- Analyser pourquoi ces chaînes ont un effet négatif\n")
  cat("- Améliorer la qualité du contenu ou remplacer\n")
  cat("- Réduire leur visibilité dans l'offre de base\n")
}
```

# 12 Approximate Bayesian Computation (ABC)

## Introduction

L'**Approximate Bayesian Computation (ABC)** est une famille de méthodes d'inférence bayésienne utilisée lorsque :
- La **vraisemblance** $f(Y|\theta)$ est difficile ou **impossible à calculer** analytiquement
- Mais on peut **simuler facilement** des données depuis le modèle

**Principe fondamental** : Au lieu de calculer la vraisemblance, on génère des données simulées et on accepte les paramètres qui produisent des données "proches" des observations réelles.

## Rappel théorique

### Algorithme ABC standard

Pour j = 1 à N (nombre d'échantillons souhaités) :
  Répéter :
    1. Générer $\theta^{*} \sim \pi (\theta)$           # Tirage depuis la loi a priori
    2. Simuler $Y^{*} \sim f(\cdot \mid \theta^{*})$         # Simulation de données
    3. Calculer distance $\rho (S(Y^{*}), S(Y))$   # Comparaison via statistiques résumées
  Jusqu'à ce que $\rho (S(Y^{*}), S(Y)) ≤ \epsilon$ 
  
  Retenir $\theta^{*}(j) = \theta^{*}$
Fin

**Composantes clés** :

| Composante | Description | Rôle |
|------------|-------------|------|
| $\pi(\theta)$ | Loi a priori | Espace de recherche initial |
| $S(.)$ | Statistique(s) résumée(s) | Réduction de dimension |
| $\rho(.,.)$ | Fonction de distance | Mesure de proximité |
| $\varepsilon$ | Seuil (tolerance) | Contrôle précision/efficacité |

### Justification théorique

On peut montrer que l'ABC échantillonne approximativement depuis :

$$\pi_{ABC}(\theta) = \pi(\theta | \rho(S(Y), S(Y^*)) \leq \varepsilon)$$

**Limite** : Quand $\varepsilon \to 0$ et si $S(Y)$ est une statistique **suffisante**, alors :
$$\pi_{ABC}(\theta) \to \pi(\theta | Y)$$

### Application au modèle de régression

Pour notre modèle :
$$Y = \mu \mathbf{1} + X\beta + \epsilon, \quad \epsilon \sim \mathcal{N}(0, \sigma^2_\epsilon I)$$

- **Paramètres** : $\theta = (\mu, \beta, \sigma^2_\epsilon)$
- **Vraisemblance** : $f(Y|\theta) = (2\pi\sigma^2_\epsilon)^{-n/2} \exp\left(-\frac{1}{2\sigma^2_\epsilon}\|Y - \mu\mathbf{1} - X\beta\|^2\right)$

**Note** : Pour la régression linéaire, la vraisemblance est **facile** à calculer $\to$ ABC n'est pas optimal ici. Mais c'est un excellent exercice pédagogique !

## 12.1 Implémentation ABC standard

### Version simplifiée : estimation de ($\mu, \sigma^2$) avec $\beta$ fixé

```{r abc_function_simple}
# Fonction ABC pour estimer mu et sigma²_epsilon
# On fixe beta pour simplifier (dimension réduite : 2 paramètres au lieu de p+2)

abc_regression_simple <- function(Y, X, beta_fixed, 
                                  n_accept = 1000, 
                                  epsilon = 0.5,
                                  prior_mu_range = c(-10, 10),
                                  prior_sigma2_range = c(0.1, 10),
                                  verbose = TRUE) {
  
  n <- length(Y)
  
  # Statistiques résumées des données observées
  # On choisit : moyenne, variance, quartiles
  S_obs <- c(
    mean(Y), 
    var(Y), 
    quantile(Y, c(0.25, 0.75))
  )
  
  # Stockage des paramètres acceptés
  accepted_params <- matrix(NA, nrow = n_accept, ncol = 2)
  colnames(accepted_params) <- c("mu", "sigma2")
  
  # Compteurs
  n_total <- 0
  n_accepted <- 0
  
  if(verbose) cat("=== Lancement ABC standard ===\n")
  if(verbose) cat("Objectif :", n_accept, "échantillons acceptés\n")
  if(verbose) cat("Seuil epsilon :", epsilon, "\n\n")
  
  # Boucle principale
  while(n_accepted < n_accept) {
    n_total <- n_total + 1
    
    # Étape 1 : Générer paramètres depuis a priori uniforme
    mu_star <- runif(1, prior_mu_range[1], prior_mu_range[2])
    sigma2_star <- runif(1, prior_sigma2_range[1], prior_sigma2_range[2])
    
    # Étape 2 : Simuler données Y* depuis le modèle
    Y_star <- mu_star + X %*% beta_fixed + rnorm(n, mean = 0, sd = sqrt(sigma2_star))
    
    # Étape 3 : Calculer statistiques résumées de Y*
    S_star <- c(
      mean(Y_star),
      var(Y_star),
      quantile(Y_star, c(0.25, 0.75))
    )
    
    # Étape 4 : Calculer distance euclidienne
    distance <- sqrt(sum((S_obs - S_star)^2))
    
    # Étape 5 : Critère d'acceptation
    if(distance <= epsilon) {
      n_accepted <- n_accepted + 1
      accepted_params[n_accepted, ] <- c(mu_star, sigma2_star)
      
      # Affichage progression
      if(verbose && n_accepted %% 100 == 0) {
        cat("Accepté:", n_accepted, "/", n_accept, 
            "| Taux:", round(100*n_accepted/n_total, 2), "%",
            "| Total simulations:", n_total, "\n")
      }
    }
    
    # Limite de sécurité
    if(n_total > 1e6) {
      warning("Nombre maximum d'itérations atteint (1 million).")
      warning("Essayez d'augmenter epsilon ou d'élargir les a priori.")
      break
    }
  }
  
  if(verbose) {
    cat("\n=== ABC terminé ===\n")
    cat("Taux d'acceptation final:", round(100*n_accepted/n_total, 2), "%\n")
    cat("Nombre total de simulations:", format(n_total, big.mark=" "), "\n")
  }
  
  return(list(
    params = as.data.frame(accepted_params[1:n_accepted, ]),
    acceptance_rate = n_accepted / n_total,
    n_simulations = n_total,
    epsilon = epsilon
  ))
}
```

### Application sur nos données

```{r abc_application}
# Chargement des données (supposées déjà disponibles)
# Y_train, X_train, beta_hat_bayesA

# On utilise les beta estimés par Bayes A comme "vérité"
beta_fixed <- beta_hat_bayesA

# Lancement ABC
set.seed(456)
abc_result <- abc_regression_simple(
  Y = Y_train,
  X = X_train,
  beta_fixed = beta_fixed,
  n_accept = 1000,
  epsilon = 2.0,  # À ajuster selon les résultats
  prior_mu_range = c(-5, 5),
  prior_sigma2_range = c(0.1, 20)
)
```

### Analyse des résultats

```{r abc_results_analysis}
# Statistiques descriptives des distributions a posteriori
abc_summary <- data.frame(
  Parametre = c("$\\mu$", "$\\sigma^2_{\\epsilon}$"),
  Moyenne_posteriori = c(
    mean(abc_result$params$mu),
    mean(abc_result$params$sigma2)
  ),
  Mediane_posteriori = c(
    median(abc_result$params$mu),
    median(abc_result$params$sigma2)
  ),
  Ecart_type = c(
    sd(abc_result$params$mu),
    sd(abc_result$params$sigma2)
  ),
  IC_95_inf = c(
    quantile(abc_result$params$mu, 0.025),
    quantile(abc_result$params$sigma2, 0.025)
  ),
  IC_95_sup = c(
    quantile(abc_result$params$mu, 0.975),
    quantile(abc_result$params$sigma2, 0.975)
  )
)

# Ajout comparaison avec Bayes A
abc_summary$Bayes_A <- c(mu_hat_bayesA, sigma2_e_bayesA)

kable(
  abc_summary,
  digits = 4,
  escape = FALSE,   # indispensable pour afficher le LaTeX
  caption = "Distributions a posteriori ABC vs estimations Bayes A"
) %>%
  kable_styling(latex_options = "hold_position")
```

```{r plot_abc_distributions}
library(ggplot2)
library(gridExtra)

# Distribution de mu
p1 <- ggplot(abc_result$params, aes(x = mu)) +
  geom_histogram(aes(y = ..density..), bins = 30, 
                 fill = "steelblue", alpha = 0.7, color = "white") +
  geom_density(color = "darkblue", size = 1.2) +
  geom_vline(xintercept = mu_hat_bayesA, 
             color = "red", linetype = "dashed", size = 1.2) +
  geom_vline(xintercept = mean(abc_result$params$mu),
             color = "darkgreen", linetype = "solid", size = 1.2) +
  annotate("text", x = mu_hat_bayesA, y = Inf, 
           label = "Bayes A", vjust = 2, hjust = -0.1, color = "red") +
  annotate("text", x = mean(abc_result$params$mu), y = Inf,
           label = "ABC", vjust = 2, hjust = 1.1, color = "darkgreen") +
  labs(title = expression("Distribution a posteriori de" * mu * "(ABC)"),
       x = expression(mu), y = "Densité") +
  theme_minimal()

# Distribution de sigma²
p2 <- ggplot(abc_result$params, aes(x = sigma2)) +
  geom_histogram(aes(y = ..density..), bins = 30,
                 fill = "coral", alpha = 0.7, color = "white") +
  geom_density(color = "darkred", size = 1.2) +
  geom_vline(xintercept = sigma2_e_bayesA,
             color = "red", linetype = "dashed", size = 1.2) +
  geom_vline(xintercept = mean(abc_result$params$sigma2),
             color = "darkgreen", linetype = "solid", size = 1.2) +
  annotate("text", x = sigma2_e_bayesA, y = Inf,
           label = "Bayes A", vjust = 2, hjust = -0.1, color = "red") +
  annotate("text", x = mean(abc_result$params$sigma2), y = Inf,
           label = "ABC", vjust = 2, hjust = 1.1, color = "darkgreen") +
  labs(title = expression("Distribution a posteriori de " * sigma^2 * "_" * epsilon * " (ABC)"),
       x = expression(sigma^2 * "_" * epsilon), y = "Densité") +
  theme_minimal()

gridExtra::grid.arrange(p1, p2, ncol = 1)
```

**Interprétation** :

1. **Convergence** : Les moyennes ABC sont-elles proches de Bayes A ?
   - Si oui $\to$ bon choix de $\epsilon$ et de statistiques
   - Si non $\to \epsilon$ trop grand (biais) ou statistiques insuffisantes

2. **Incertitude** : Les distributions ABC sont plus larges que Bayes A
   - Normal : perte d'information due à l'approximation
   - Les statistiques résumées ne capturent pas toute l'information

3. **Taux d'acceptation** : $\sim$ `r round(100*abc_result$acceptance_rate, 2)`%
   - Si < 1% $\to \epsilon$ trop strict, calcul très long
   - Si > 10% $\to \epsilon$ peut-être trop généreux

## 12.2 ABC avec test d'adéquation (modification demandée)

**Modification** : Au lieu d'utiliser une distance euclidienne sur des statistiques résumées, on utilise un **test statistique** pour comparer directement les distributions.

### Test de Kolmogorov-Smirnov

Le test KS compare deux échantillons et teste l'hypothèse :
- **$H_{0}$** : Les deux échantillons proviennent de la même distribution
- **$H_{1}$** : Les distributions sont différentes

**Critère d'acceptation** : On accepte θ* si **p-value > seuil** (distributions non significativement différentes)

```{r abc_ks_function}
# ABC avec test de Kolmogorov-Smirnov

abc_regression_ks <- function(Y, X, beta_fixed,
                              n_accept = 1000,
                              ks_pvalue_threshold = 0.05,
                              prior_mu_range = c(-10, 10),
                              prior_sigma2_range = c(0.1, 10),
                              verbose = TRUE) {
  
  n <- length(Y)
  
  # Stockage
  accepted_params <- matrix(NA, nrow = n_accept, ncol = 2)
  colnames(accepted_params) <- c("mu", "sigma2")
  ks_pvalues <- numeric(n_accept)
  
  n_total <- 0
  n_accepted <- 0
  
  if(verbose) cat("=== Lancement ABC avec test KS ===\n")
  if(verbose) cat("Seuil p-value :", ks_pvalue_threshold, "\n\n")
  
  while(n_accepted < n_accept) {
    n_total <- n_total + 1
    
    # 1. Générer paramètres
    mu_star <- runif(1, prior_mu_range[1], prior_mu_range[2])
    sigma2_star <- runif(1, prior_sigma2_range[1], prior_sigma2_range[2])
    
    # 2. Simuler Y*
    Y_star <- mu_star + X %*% beta_fixed + rnorm(n, mean = 0, sd = sqrt(sigma2_star))
    
    # 3. Test de Kolmogorov-Smirnov entre Y et Y*
    ks_test <- ks.test(Y, Y_star)
    
    # 4. Critère d'acceptation : p-value > seuil
    # Si p-value élevée → distributions similaires → accepter
    if(ks_test$p.value > ks_pvalue_threshold) {
      n_accepted <- n_accepted + 1
      accepted_params[n_accepted, ] <- c(mu_star, sigma2_star)
      ks_pvalues[n_accepted] <- ks_test$p.value
      
      if(verbose && n_accepted %% 100 == 0) {
        cat("Accepté:", n_accepted, "/", n_accept,
            "| Taux:", round(100*n_accepted/n_total, 2), "%",
            "| Simulations:", n_total, "\n")
      }
    }
    
    if(n_total > 1e6) {
      warning("Limite d'itérations atteinte.")
      break
    }
  }
  
  if(verbose) {
    cat("\n=== ABC-KS terminé ===\n")
    cat("Taux d'acceptation final:", round(100*n_accepted/n_total, 2), "%\n")
    cat("P-values moyennes des acceptés:", round(mean(ks_pvalues[1:n_accepted]), 3), "\n")
  }
  
  return(list(
    params = as.data.frame(accepted_params[1:n_accepted, , drop = FALSE]),
    ks_pvalues = ks_pvalues[1:n_accepted],
    acceptance_rate = n_accepted / n_total,
    n_simulations = n_total,
    threshold = ks_pvalue_threshold
  ))
}
```

### Application du test KS

```{r abc_ks_application}
set.seed(789)
abc_ks_result <- abc_regression_ks(
  Y = Y_train,
  X = X_train,
  beta_fixed = beta_fixed,
  n_accept = 500,  # Moins car test plus strict
  ks_pvalue_threshold = 0.1,  # Ajuster selon résultats
  prior_mu_range = c(-5, 5),
  prior_sigma2_range = c(0.1, 20)
)
```

### Comparaison ABC standard vs ABC-KS

```{r comparison_abc_methods}
# Tableau comparatif
comparison_table <- data.frame(
  Méthode = c("ABC standard (distance)", "ABC test KS", "Bayes A (référence)"),
  mu_moyen = c(
    mean(abc_result$params$mu),
    mean(abc_ks_result$params$mu),
    mu_hat_bayesA
  ),
  mu_ET = c(
    sd(abc_result$params$mu),
    sd(abc_ks_result$params$mu),
    NA
  ),
  sigma2_moyen = c(
    mean(abc_result$params$sigma2),
    mean(abc_ks_result$params$sigma2),
    sigma2_e_bayesA
  ),
  sigma2_ET = c(
    sd(abc_result$params$sigma2),
    sd(abc_ks_result$params$sigma2),
    NA
  ),
  Taux_acceptation_pct = c(
    round(100 * abc_result$acceptance_rate, 2),
    round(100 * abc_ks_result$acceptance_rate, 2),
    NA
  )
)

kable(comparison_table, digits = 3,
      caption = "Comparaison ABC standard vs ABC test KS") %>%
  kable_styling(latex_options = "hold_position")
```

```{r plot_comparison_methods}
# Combinaison des résultats
abc_combined <- rbind(
  data.frame(
    mu = abc_result$params$mu,
    sigma2 = abc_result$params$sigma2,
    Méthode = "ABC standard"
  ),
  data.frame(
    mu = abc_ks_result$params$mu,
    sigma2 = abc_ks_result$params$sigma2,
    Méthode = "ABC test KS"
  )
)

# Graphique comparatif pour mu
p1 <- ggplot(abc_combined, aes(x = mu, fill = Méthode)) +
  geom_density(alpha = 0.5) +
  geom_vline(xintercept = mu_hat_bayesA, 
             linetype = "dashed", color = "red", size = 1) +
  annotate("text", x = mu_hat_bayesA, y = Inf,
           label = "Bayes A", vjust = 2, color = "red") +
  labs(title = expression("Distributions a posteriori de " * mu * " : comparaison"),
       x = expression(mu), y = "Densité") +
  scale_fill_manual(values = c("ABC standard" = "steelblue", 
                                "ABC test KS" = "coral")) +
  theme_minimal() +
  theme(legend.position = "bottom")

# Graphique comparatif pour sigma²
p2 <- ggplot(abc_combined, aes(x = sigma2, fill = Méthode)) +
  geom_density(alpha = 0.5) +
  geom_vline(xintercept = sigma2_e_bayesA,
             linetype = "dashed", color = "red", size = 1) +
  annotate("text", x = sigma2_e_bayesA, y = Inf,
           label = "Bayes A", vjust = 2, color = "red") +
  labs(title = expression("Distributions a posteriori de " * sigma^2 * "_" * epsilon * " : comparaison"),
       x = "σ²_ε", y = "Densité") +
  scale_fill_manual(values = c("ABC standard" = "steelblue",
                                "ABC test KS" = "coral")) +
  theme_minimal() +
  theme(legend.position = "bottom")

gridExtra::grid.arrange(p1, p2, ncol = 1)
```

### Analyse comparative

**1. Taux d'acceptation**
- ABC standard : $\sim$ `r round(100*abc_result$acceptance_rate, 2)`%
- ABC test KS : $\sim$ `r round(100*abc_ks_result$acceptance_rate, 2)`%
- Le test KS est généralement **plus strict** $\to$ taux plus faible

**2. Précision des estimations**
- Les deux méthodes convergent vers des valeurs similaires
- ABC-KS peut être légèrement biaisé si le test est trop conservateur

**3. Interprétation**
- **ABC standard** : contrôle direct via $\epsilon$ (paramètre arbitraire)
- **ABC test KS** : contrôle via p-value (interprétation statistique claire)

**4. Avantages du test KS**
- Pas besoin de choisir des statistiques résumées
- Interprétation statistique (p-value)
- Utilise toute l'information de la distribution empirique

**5. Limites du test KS**
- Peut être trop conservateur (rejette trop)
- Moins puissant pour détecter certaines différences fines
- Sensible à la taille de l'échantillon

---

## 12.3 Choix du seuil ($\epsilon$ ou $p-value$)

### Impact de ε sur ABC standard

```{r epsilon_sensitivity, eval=FALSE}
# Test de sensibilité à epsilon
epsilons <- c(0.5, 1.0, 2.0, 5.0)
results_by_epsilon <- list()

for(eps in epsilons) {
  results_by_epsilon[[as.character(eps)]] <- abc_regression_simple(
    Y = Y_train, X = X_train, beta_fixed = beta_fixed,
    n_accept = 500, epsilon = eps, verbose = FALSE
  )
}

# Comparaison
epsilon_comparison <- do.call(rbind, lapply(names(results_by_epsilon), function(eps) {
  res <- results_by_epsilon[[eps]]
  data.frame(
    Epsilon = as.numeric(eps),
    mu_moyen = mean(res$params$mu),
    sigma2_moyen = mean(res$params$sigma2),
    Taux_acceptation = round(100 * res$acceptance_rate, 2)
  )
}))

kable(epsilon_comparison, digits = 3,
      caption = "Sensibilité au choix de epsilon") %>%
  kable_styling(latex_options = "hold_position")
```

**Observations** :
- **$\epsilon$ petit** : estimations précises mais peu d'acceptations (calcul long)
- **$\epsilon$ grand** : beaucoup d'acceptations mais biais potentiel
- **Compromis optimal** : taux d'acceptation entre 1-5%

---

## 12.4 Avantages et limites de l'ABC

### Avantages

1. **Pas besoin de vraisemblance explicite**
   - Applicable à des modèles très complexes
   - Modèles avec équations différentielles, processus stochastiques
   - Modèles multi-échelles, agent-based models

2. **Simulation facile**
   - Si on peut simuler depuis le modèle → ABC applicable
   - Plus simple que dériver analytiquement la vraisemblance

3. **Interprétation intuitive**
   - Principe : "accepter les paramètres qui génèrent des données similaires"
   - Accessible conceptuellement

4. **Flexibilité**
   - Choix libre des statistiques résumées
   - Choix libre de la distance
   - Modifications faciles (test KS, autres tests)

### Limites

1. **Efficacité computationnelle catastrophique en haute dimension**
   - Taux d'acceptation : $\sim$ $\varepsilon^p$ où p = dimension
   - Notre exemple 2D : 1-5% acceptable
   - Dimension 10 : < 0.001% $\to$ millions de simulations
   - Dimension 160 (nos $\beta$) : **impraticable**

2. **Choix des statistiques résumées (crucial)**
   - Statistiques **non suffisantes** $\to$ perte d'information
   - Pas de méthode universelle pour les choisir
   - Compromis dimension/information

3. **Choix du seuil $\epsilon$ (difficile)**
   - Pas de règle optimale
   - $\epsilon$ petit : précis mais coûteux
   - $\epsilon$ grand : rapide mais biaisé
   - Nécessite souvent plusieurs essais

4. **Approximation (pas exact)**
   - ABC donne $\pi(\theta | \rho(S(Y), S(Y^*)) < \varepsilon)$
   - Ce n'est **pas** exactement $\pi(\theta | Y)$
   - Qualité dépend de ε et des statistiques

5. **Validation difficile**
   - Comment savoir si on a convergé ?
   - Pas de diagnostic comme pour MCMC

### Quand utiliser ABC ?

**On utiliser ABC lorsque :**
- Vraisemblance intractable ou inconnue
- Modèles génératifs complexes
- Prototypage rapide d'un modèle exploratoire
- Dimension faible (< 10 paramètres)

**On ne utilise pas ABC lorsque :**
- Vraisemblance facile à calculer (notre cas !)
- Haute dimension (> 20 paramètres)
- Besoin de précision maximale
- Contraintes de temps de calcul

## 12.5 Améliorations de l'ABC

### ABC-SMC (Sequential Monte Carlo)

**Principe** : Réduire progressivement $\epsilon$

1. Commencer avec $\epsilon_1$ grand
2. Accepter N particules
3. Réduire $\epsilon_2 < \epsilon_1$
4. Perturber les particules acceptées
5. Accepter selon nouveau seuil
6. Répéter jusqu'à $\epsilon_final$ petit


**Avantages** : Meilleur taux d'acceptation global

### ABC-MCMC

**Principe** : Intégrer ABC dans Metropolis-Hastings

Proposer $\theta^{*}$ depuis $q(\cdot \mid \theta_t)$
Simuler $Y^{*} \sim f(\cdot \mid \theta^{*})$
Si $\rho(S(Y), S(Y^{*})) < \epsilon$:
  Accepter avec proba $\min\left(1,\; \frac{\pi(\theta^{*})}{\pi(\theta_{t})} \times \frac{q(\theta_{t}\mid \theta^{*})}{q(\theta^{*}\mid \theta_{t})}\right)$
Sinon :
  Rejeter

### Régression ABC

**Principe** : Correction post-hoc des échantillons acceptés

1. Accepter avec ε large (beaucoup d'échantillons)
2. Faire une régression : $\theta \sim S(Y^*)$
3. Prédire $\hat{\theta}$ pour $S(Y)$ observé
4. Ajuster localement

### Sélection automatique de statistiques

**Méthodes** :
- Réseaux de neurones pour apprendre les statistiques optimales
- Analyse en composantes principales sur statistiques candidates
- Minimisation d'une fonction de perte

## 12.6 Conclusion sur l'ABC

### Pour notre problème de régression

L'ABC est **pédagogiquement intéressant** mais **pratiquement sous-optimal** :

| Critère | ABC | Gibbs (Bayes A) | Recommandation |
|---------|-----|-----------------|----------------|
| Vraisemblance | Non nécessaire | Nécessaire | Disponible → Gibbs |
| Dimension | Difficile (p=160) | OK | Gibbs |
| Temps calcul | Très long | Moyen | Gibbs |
| Précision | Approximative | Exacte | Gibbs |

**Verdict** : Pour la régression linéaire, **préférer les méthodes MCMC classiques**

### Apport de la modification (test KS)

*Avantages** :
- Interprétation statistique claire (p-value)
- Pas de choix arbitraire de statistiques résumées
- Utilise toute la distribution

*Inconvénients** :
- Peut être trop conservateur
- Taux d'acceptation souvent plus faible
- Pas nécessairement plus précis

**Comparaison** :

| Aspect | ABC standard | ABC test KS |
|--------|--------------|-------------|
| Critère | Distance $\rho$ | P-value test |
| Statistiques | À choisir | Distribution complète |
| Interprétation | Arbitraire | Statistique |
| Efficacité | Variable | Souvent plus faible |

**Recommandation** : Le test KS est une amélioration **conceptuelle** intéressante mais pas nécessairement plus efficace en pratique.

### Cas d'usage réels de l'ABC

1. **Génétique des populations**
   - Histoire démographique complexe
   - Mutations, recombinaisons, sélection

2. **Épidémiologie**
   - Modèles SIR/SEIR complexes
   - Réseaux de contacts

3. **Écologie**
   - Dynamiques de populations
   - Modèles proie-prédateur